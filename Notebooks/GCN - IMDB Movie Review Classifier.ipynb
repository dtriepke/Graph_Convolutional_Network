{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-based Graph Convolutional Network\n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "- https://arxiv.org/pdf/1809.05679.pdf  \n",
    "- https://arxiv.org/pdf/1609.02907.pdf  \n",
    "- https://github.com/plkmo/Bible_Text_GCN   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import pickle\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def nCr(n, r):\n",
    "    f = math.factorial\n",
    "    return int(f(n)/(f(r)*f(n-r)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FILE PATH adjusted for local\n",
    "def save_as_pickle(filename, data):\n",
    "    completeName = os.path.join(\"../data\", filename)\n",
    "    print(completeName)\n",
    "    with open(completeName, 'wb') as output:\n",
    "        pickle.dump(data, output)\n",
    "        \n",
    "def load_pickle(filename):\n",
    "    completeName = os.path.join(\"../data\", filename)\n",
    "    with open(completeName, 'rb') as pkl_file:\n",
    "        data = pickle.load(pkl_file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(load_from_disc, debug, test_size = None, train_size = None, vocab_size = None):\n",
    "    \"\"\"\n",
    "    load_from_disc: load data from s3 or from own disk; the later ther must be stored priorly, \n",
    "    debug: defines if the train and test data filtered, \n",
    "    test_size: size of the test data  (Debug = True)\n",
    "    train_size = size of the train data  (Debug = True)\n",
    "    vocab_size = size of the vocabolary (load_from_disc = False)\n",
    "    \"\"\"\n",
    "    if load_from_disc:\n",
    "        docs = load_pickle(\"data_joined.pkl\")\n",
    "        y_train = load_pickle(\"y_train.pkl\")\n",
    "        y_test = load_pickle(\"y_test.plk\")\n",
    "\n",
    "        # Index for semi supervised test data \n",
    "        idx_test = [i for i in range(len(y_train), len(y_train) + len(y_test) )]\n",
    "        idx_train = [i for i in range(len(y_train))]\n",
    "\n",
    "        if debug:\n",
    "            # DEBUG!!!\n",
    "            idx_train = idx_train[:train_size]\n",
    "            idx_test = idx_test[:test_size] \n",
    "\n",
    "            X_train = [docs[i] for i in idx_train]\n",
    "            X_test = [docs[i] for i in idx_test]\n",
    "            docs = np.append(X_train, X_test)\n",
    "\n",
    "            y_train = [y_train[i] for i in [idx_train]]\n",
    "            y_test = y_test[:test_size]\n",
    "\n",
    "\n",
    "    else:\n",
    "        vocab_size = vocab_size\n",
    "        imdb = keras.datasets.imdb\n",
    "\n",
    "        (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)\n",
    "\n",
    "        if debug:\n",
    "            # DEBUG!!!\n",
    "            X_train = X_train[:train_size]\n",
    "            y_train = y_train[:train_size]\n",
    "\n",
    "            X_test = X_test[:5]\n",
    "            y_test = y_test[:5]\n",
    "\n",
    "        # Decode imdb index to words\n",
    "        imdb_word_index = imdb.get_word_index()\n",
    "        imdb_word_index = {k: (v + 3) for k,v in imdb_word_index.items()}\n",
    "        imdb_word_index[\"<PAD>\"] = 0\n",
    "        imdb_word_index[\"<START>\"] = 1\n",
    "        imdb_word_index[\"<UNK>\"] = 2\n",
    "        imdb_word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "        imdb_index_word = {idx: value for value, idx in imdb_word_index.items()}\n",
    "\n",
    "        def decode_text(txt):\n",
    "            return ' '.join([imdb_index_word.get(i, '?') for i in txt])\n",
    "\n",
    "\n",
    "        # Create data for semi-supervised classifier\n",
    "        data = np.append(X_train, X_test)\n",
    "        docs = list(map(lambda i: decode_text(i), data )) # decode idx to text\n",
    "\n",
    "\n",
    "        # Index for semi supervised test data \n",
    "        idx_test = [i for i in range(len(y_train), len(y_train) + len(y_test) )]\n",
    "        idx_train = [i for i in range(len(y_train))]\n",
    "\n",
    "        # Store results\n",
    "        save_as_pickle(\"data_joined.pkl\", docs)\n",
    "        save_as_pickle(\"y_train.pkl\", y_train)\n",
    "        save_as_pickle(\"y_test.plk\", y_test)\n",
    "    \n",
    "\n",
    "    return docs, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs, y_train, y_test = load_data(load_from_disc = True, debug = True, train_size = 10000, test_size = 10000, vocab_size = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Graph Edges\n",
    "****\n",
    "\n",
    "<img src=\"weights_formula.png\" alt=\"drawing\" width=\"500\">\n",
    "\n",
    "\n",
    "**PMI**  \n",
    "\n",
    "* PMI is the *Point-wise Mutual Information* between pairs of co-occurring words over a sliding window $\\#W$ that we fix to be of 10-words length\n",
    "* $\\#W(i)$ is the number of sliding windows in a corpus that contain word $i$  \n",
    "* $\\#W(i,j)$ is the number of sliding windows that contain both word $i$ and $j$  \n",
    "* $\\#W$ is the total number of sliding windows in the corpus\n",
    "\n",
    "\n",
    "**TF-iDF** \n",
    "For the word-document relationship in the graph the author proposed the tf-idf method. As this is the easer information and a well known method we will start here.\n",
    "\n",
    "\n",
    "\n",
    "## TF-iDF\n",
    "***\n",
    "\n",
    "Our target is to create a data frame with the dimension [num docs x vocabulary size] containing the TF-iDF score for each word and document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>about</th>\n",
       "      <th>acting</th>\n",
       "      <th>actors</th>\n",
       "      <th>actually</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>...</th>\n",
       "      <th>why</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065817</td>\n",
       "      <td>0.031299</td>\n",
       "      <td>0.022770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040497</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>0.019020</td>\n",
       "      <td>0.020756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023753</td>\n",
       "      <td>0.033274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49995</td>\n",
       "      <td>0.013708</td>\n",
       "      <td>0.051403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022687</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>0.007849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.010895</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.074594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101435</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030673</td>\n",
       "      <td>0.022314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             10     about    acting  actors  actually     after     again  \\\n",
       "0      0.000000  0.000000  0.000000     0.0  0.000000  0.032836  0.000000   \n",
       "1      0.000000  0.000000  0.033705     0.0  0.039557  0.000000  0.040497   \n",
       "2      0.000000  0.000000  0.049408     0.0  0.057986  0.000000  0.000000   \n",
       "3      0.000000  0.007552  0.000000     0.0  0.000000  0.009977  0.000000   \n",
       "4      0.000000  0.000000  0.000000     0.0  0.047667  0.000000  0.000000   \n",
       "...         ...       ...       ...     ...       ...       ...       ...   \n",
       "49995  0.013708  0.051403  0.000000     0.0  0.000000  0.011318  0.000000   \n",
       "49996  0.000000  0.000000  0.000000     0.0  0.000000  0.000000  0.000000   \n",
       "49997  0.000000  0.000000  0.079427     0.0  0.000000  0.000000  0.000000   \n",
       "49998  0.000000  0.000000  0.000000     0.0  0.000000  0.000000  0.000000   \n",
       "49999  0.000000  0.000000  0.000000     0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "            all      also        an  ...      why      will      with  work  \\\n",
       "0      0.065817  0.031299  0.022770  ...  0.00000  0.000000  0.036056   0.0   \n",
       "1      0.021971  0.000000  0.022803  ...  0.00000  0.000000  0.018055   0.0   \n",
       "2      0.000000  0.000000  0.066854  ...  0.00000  0.000000  0.026466   0.0   \n",
       "3      0.006666  0.019020  0.020756  ...  0.00000  0.009604  0.032867   0.0   \n",
       "4      0.026476  0.000000  0.027479  ...  0.04577  0.000000  0.021757   0.0   \n",
       "...         ...       ...       ...  ...      ...       ...       ...   ...   \n",
       "49995  0.022687  0.010789  0.007849  ...  0.00000  0.010895  0.012428   0.0   \n",
       "49996  0.135261  0.000000  0.000000  ...  0.00000  0.000000  0.000000   0.0   \n",
       "49997  0.051777  0.000000  0.053737  ...  0.00000  0.074594  0.000000   0.0   \n",
       "49998  0.000000  0.000000  0.000000  ...  0.00000  0.000000  0.000000   0.0   \n",
       "49999  0.000000  0.030673  0.022314  ...  0.00000  0.000000  0.070671   0.0   \n",
       "\n",
       "          world     would     years       you     young      your  \n",
       "0      0.000000  0.028304  0.000000  0.087607  0.000000  0.000000  \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2      0.000000  0.000000  0.000000  0.000000  0.000000  0.053613  \n",
       "3      0.012950  0.000000  0.023753  0.033274  0.000000  0.000000  \n",
       "4      0.000000  0.000000  0.000000  0.052863  0.000000  0.000000  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "49995  0.000000  0.000000  0.000000  0.007549  0.000000  0.000000  \n",
       "49996  0.087587  0.000000  0.000000  0.045010  0.000000  0.000000  \n",
       "49997  0.000000  0.066797  0.000000  0.000000  0.101435  0.000000  \n",
       "49998  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "49999  0.000000  0.000000  0.000000  0.021464  0.000000  0.000000  \n",
       "\n",
       "[50000 rows x 192 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(input = \"content\")\n",
    "tfidf_vector = tfidf.fit(docs) \n",
    "df_tfidf = tfidf_vector.transform(docs)\n",
    "df_tfidf = df_tfidf.toarray()\n",
    "\n",
    "# Transform to a data frame\n",
    "vocab = tfidf.get_feature_names(); vocab = np.array(vocab)\n",
    "df_tfidf = pd.DataFrame(df_tfidf, columns=vocab)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vacabolary size 192\n"
     ]
    }
   ],
   "source": [
    "print(\"Vacabolary size %.f\" % vocab.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point-wise Mutual Information between words\n",
    "\n",
    "***\n",
    "\n",
    "For the PMI we run first the sliding window over the document and count the windows for occurencies of a word and word pairs. \n",
    "Next we calculate the relative frequency $p(i)$ and $p(i,j)$ and finaly get the PMI score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18336/18336 [00:00<00:00, 162698.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# Filter the docs by by all word including in the vocab (tfidf applies a stop word remover)\n",
    "names = vocab # vocab from tf-idf calculation\n",
    "docs_filtered = list(map(lambda x: \" \".join([w for w in x.split() if w in names]), docs)) \n",
    "\n",
    "# Dictionaries and lookups\n",
    "word_index = OrderedDict( (name, index) for index, name in enumerate(names) )\n",
    "index_word = OrderedDict( (idx, name )  for idx, name in enumerate(names) ) \n",
    "\n",
    "word_word_index = OrderedDict()\n",
    "# index_word_word = OrderedDict()\n",
    "\n",
    "for w1, w2 in tqdm(combinations(names, 2), total = nCr(len(names), 2)):\n",
    "    word_word = \"{}_{}\".format(w1,w2)\n",
    "    word_word_index[word_word] = len(word_word_index)\n",
    "#     index_word_word[len(word_word_index)] = word_word\n",
    "    \n",
    "\n",
    "# This function help later to access the word word dictionary because for w1_w2 = w2_w1 is not both represented\n",
    "def get_word_word_index(w1, w2):\n",
    "    word_word = \"{}_{}\".format(w1, w2)\n",
    "    idx = word_word_index.get(word_word, \"?\")\n",
    "    \n",
    "    if  idx == \"?\":\n",
    "        word_word = \"{}_{}\".format(w2, w1)\n",
    "        idx = word_word_index.get(word_word, \"?\")\n",
    "    \n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [07:12<00:00, 115.70it/s]\n",
      "  0%|                                                                                                                                                                                                                                              | 0/18336 [00:00<?, ?it/s]C:\\Users\\dtrie\\Anaconda3\\envs\\tfEnv\\lib\\site-packages\\ipykernel\\__main__.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18336/18336 [00:00<00:00, 24222.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Find the co-occurrences:\n",
    "window_size = 10 # sliding window size to calculate point-wise mutual information between words\n",
    "\n",
    "# Window counter\n",
    "W = 0 # total number of sliding windows in the corpus\n",
    "W_ij = np.zeros(len(word_word_index), dtype = np.int32) # OrderedDict((ww, 0) for ww in word_word_index.items()) # co-occurencies of word i and word j\n",
    "W_i  = OrderedDict((name, 0) for name in names) # occurencies of word i\n",
    "\n",
    "\n",
    "for doc in tqdm(docs_filtered, total = len(docs_filtered)):\n",
    "    \n",
    "    # In each doc run a sliding window with size 10\n",
    "    doc = doc.split()\n",
    "    for idx in range(len(doc) - window_size):\n",
    "        W += 1 # count total windows\n",
    "        \n",
    "        words = set(doc[idx:(idx + window_size)]) # distict list of words in the current window\n",
    "    \n",
    "        # count windows conaining word i\n",
    "        for word in words:\n",
    "            W_i[word] += 1\n",
    "        \n",
    "        # count windows containing co-occurrences of words i and j \n",
    "        for i in combinations(words, 2):\n",
    "            w1 = i[0]\n",
    "            w2 = i[1]\n",
    "            idx = get_word_word_index(w1, w2)\n",
    "            W_ij[idx] += 1 # Update window counter\n",
    "\n",
    "\n",
    "# Now can we calculate the probabilities and the PMI score.\n",
    "            \n",
    "# Relative frequency\n",
    "p_ij = pd.Series(W_ij, index = word_word_index.keys()) / W\n",
    "p_i = pd.Series(W_i, index = W_i.keys()) / W\n",
    "pmi_ij = p_ij.copy() \n",
    "\n",
    "# Calculate the PMI\n",
    "for w in tqdm(combinations(names, 2), total = nCr(len(names), 2)):\n",
    "    i = w[0]\n",
    "    j = w[1] \n",
    "    word_word = \"{}_{}\".format(i, j)\n",
    "    \n",
    "    try:\n",
    "        frac = p_ij[word_word] / (p_i[i] * p_i[j])\n",
    "        frac = frac + 1E-9 \n",
    "        pmi = math.log(frac)\n",
    "        pmi_ij[word_word] = round(pmi, 4)\n",
    "    except:\n",
    "        print(\"ERROR with: \", word_word, frac)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18336/18336 [00:00<00:00, 23876.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\text_graph.pkl\n",
      ".\\data\\word_word.pkl\n",
      ".\\data\\pmi.pkl\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def word_word_edges(pmi_ij):\n",
    "    word_word_edges_list = []\n",
    "    for w in tqdm(combinations(names, 2), total = nCr(len(names), 2)):\n",
    "        i = w[0]\n",
    "        j = w[1] \n",
    "        word_word = \"{}_{}\".format(i, j)\n",
    "        pmi = pmi_ij.loc[word_word]\n",
    "        if (pmi > 0):\n",
    "            word_word_edges_list.append((i, j,{\"weight\": pmi}))\n",
    "            \n",
    "    return word_word_edges_list\n",
    "\n",
    "# Build graph with document nodes and word nodes\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(df_tfidf.index) # document nodes as index\n",
    "G.add_nodes_from(vocab) # word nodes\n",
    "\n",
    "# Build edges between document-word pairs\n",
    "document_word = [(doc, w, {\"weight\" : df_tfidf.loc[doc,w]}) for doc in df_tfidf.index for w in df_tfidf.columns if df_tfidf.loc[doc,w] > 0]\n",
    "G.add_edges_from(document_word)\n",
    "\n",
    "# Build edges between word-word pairs\n",
    "word_word = word_word_edges(pmi_ij)\n",
    "G.add_edges_from(word_word)\n",
    "\n",
    "save_as_pickle(\"text_graph.pkl\", G)\n",
    "save_as_pickle(\"word_word.pkl\", word_word)\n",
    "save_as_pickle(\"pmi.pkl\", pmi_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load graph from disk\n",
    "G = load_pickle(\"text_graph.pkl\")\n",
    "word_word = load_pickle(\"word_word.pkl\")\n",
    "pmi_ij = load_pickle(\"pmi.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Node Classification\n",
    "***\n",
    "\n",
    "Our model $f(X, A)$ is a two layer convolution network with sprectral convolutions. The model is flexible in terms of it relies just on the feature matrix $X = I$ and the adjacancy matrix $A$ of the graph structure.  The feature matrix $X$ is set as an identity matrix $I$, which simply means every word or document is represented as a one-hot vector.\n",
    "Regarding the author, using the adjacancy matrix is efficient in situations when $X$ possess to less information. \n",
    "\n",
    "\n",
    "**Two layer GCN:**\n",
    "\n",
    "First build a Adjacency and Degree\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{A}} = \\mathbf{D}^{1/2} \\mathbf{A} \\mathbf{D}^{1/2}\n",
    "$$\n",
    "\n",
    "and the layerwise linear model takes the form\n",
    "\n",
    "$$\n",
    "Z = f(X, A) = \\text{softmax} \\left( \\hat{A} \\text{ ReLu}(\\hat{A}X W_0) W_1  \\right)\n",
    "$$  \n",
    "\n",
    "where $W_0 \\in \\mathbb{R}^{C \\times H}$ and $W_1 \\in \\mathbb{R}^{H \\times F}$ are the network weights which are trained using gradient descent.\n",
    "$X$ is a one-hot encoding of each of the graph nodes. \n",
    "\n",
    "For the training of a semi-supervised classifier the authors using full dataset batch gradient descent and evaluate the cross-entropy just error over the labeled data: \n",
    "\n",
    "$$\n",
    "L = - \\sum_{l \\in \\mathbb{y}_L} \\sum_{f = 1}^{F} Y_{lf} ln Z_{lf}\n",
    "$$,\n",
    "\n",
    "with $\\mathbb{y}_L$ as set of document idices that have labels and $F$ is the dimension of the output features, which is equal to the number of classes.\n",
    "\n",
    "\n",
    "\n",
    "<img src = GCN.png width = 800>\n",
    "\n",
    "\n",
    "**Build Adjacency and Degree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adjacancy matrix\n",
    "A = nx.to_numpy_matrix(G, weight = \"weight\")\n",
    "A = A + np.eye(G.number_of_nodes()) # diag = 1\n",
    "\n",
    "# Degree matrix\n",
    "degrees = []\n",
    "for d in G.degree(weight = None):\n",
    "    if d == 0:\n",
    "        degrees.append(0)\n",
    "    else:\n",
    "        degrees.append(d[1]**(-0.5))\n",
    "degrees = np.diag(degrees)\n",
    "\n",
    "# A hat\n",
    "A_hat = degrees@A@degrees \\\n",
    "\n",
    "# Feature matrix\n",
    "X = np.eye(G.number_of_nodes()) # Features are just identity matrix\n",
    "f = X # input of the net\n",
    "\n",
    "save_as_pickle(\"f.pkl\", f)\n",
    "save_as_pickle(\"A_hat.pkl\", A_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = load_pickle(\"f.pkl\")\n",
    "A_hat = load_pickle(\"A_hat.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train \n",
    "\n",
    "For the trainig we need to tell the net work which nodes in the output layer (word - doc - embedding) is a labeled document node. As we have an semi supervised issue, some of the document embeddings have no label. The output matrix will have the shape of all nodes (words and documents) times the number of classes. Whereas the document indecies are the first. Hence, we just have to select the regarding output by the labeld node-indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim import fully_connected\n",
    "\n",
    "# Network Parameters\n",
    "n_input = f.shape[0] # Input each node in the graph as one-hot encoding\n",
    "n_classes = 2\n",
    "dropout = 0.75\n",
    "n_hidden_1 = 330 # Size of first GCN hidden weights\n",
    "n_hidden_2 = 130\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Graph inputs\n",
    "X = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "y = tf.placeholder(tf.float32,  [None, n_classes])\n",
    "\n",
    "keep_prob  = tf.placeholder(tf.float32)\n",
    "idx_selected = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "A_hat_tf = tf.convert_to_tensor(A_hat, dtype = tf.float32)\n",
    "A_hat_tf = tf.Variable(A_hat_tf)\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'h1' : tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2' : tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "}\n",
    "\n",
    "\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2' : tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "}\n",
    "\n",
    "\n",
    "def convLayer(X, A_hat_tf, w, b ):\n",
    "    X = tf.add(tf.matmul(X, w), b)  # [?,440][440, 330] + [330] = 440x330\n",
    "    X = tf.matmul(A_hat_tf, X)  # [440, 440][440,330] = [440,330]\n",
    "    return X\n",
    "\n",
    "\n",
    "def gcn(X, weights, biases, A_hat, dropout):\n",
    "\n",
    "    # First convolution layer ?x330\n",
    "    conv1 = convLayer(X, A_hat_tf, weights['h1'], biases[\"b1\"])\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.dropout(conv1)\n",
    "    # Second convolution layer 330x130\n",
    "    conv2 = convLayer(conv1, A_hat_tf, weights['h2'], biases[\"b2\"])\n",
    "    # Fully connected layer / Linear layer for logit\n",
    "    logits = fully_connected(conv2, n_classes, activation_fn = None)\n",
    "    pred = tf.nn.softmax(logits)\n",
    "    \n",
    "    return pred\n",
    "    \n",
    "\n",
    "# Build the GCN\n",
    "pred = gcn(X, weights, biases, A_hat_tf, dropout)\n",
    "\n",
    "# Filter training document nodes for semi-supervised learning\n",
    "pred = tf.gather(pred, indices = idx_selected)\n",
    "\n",
    "# Define optimizer and loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "arg_max = [tf.argmax(pred, axis = 1),  tf.argmax(y, axis = 1)]\n",
    "correct_pred = tf.equal(tf.argmax(pred, axis = 1), tf.argmax(y, axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start trainings session\n",
    "init  = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "logs = {}\n",
    "logs[\"acc\"] = []\n",
    "logs[\"loss\"] = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for e in range(5):\n",
    "        \n",
    "        batch_x = f # one hot for each node (word + docs) in the graph\n",
    "        batch_y = np.eye(n_classes)[y_train]\n",
    "        \n",
    "        sess.run(optimizer, feed_dict = {X: batch_x,\n",
    "                                         y: batch_y,\n",
    "                                         keep_prob: dropout,\n",
    "                                         idx_selected: idx_train})\n",
    "        \n",
    "        loss, acc = sess.run([cost,  accuracy], feed_dict = {X: batch_x,\n",
    "                                                            y: batch_y,\n",
    "                                                            keep_prob: 1.,\n",
    "                                                            idx_selected: idx_train})\n",
    "        \n",
    "        print(\"Epoch \", e, \"Batch size: \", batch_y.shape[0] ,\"Batch loss: \", loss, \"Training accuracy: \", acc)\n",
    "        \n",
    "        logs[\"acc\"].append(acc)\n",
    "        logs[\"loss\"].append(loss)\n",
    "\n",
    "    \n",
    "    # save_path = saver.save(sess, \"drive/My Drive/Coding/GCN/data/model.ckpt\")\n",
    "\n",
    "    print(\"Finish!\")\n",
    "\n",
    "    print(\"Store model weights and biases\")\n",
    "    weights_dict = {}\n",
    "    for key, values in weights.items():\n",
    "        weights_dict[key] = sess.run(values)\n",
    "    save_as_pickle(\"weights.pkl\", weights_dict)\n",
    "\n",
    "    biases_dict = {}\n",
    "    for key, values in biases.items():\n",
    "        biases_dict[key] = sess.run(values)\n",
    "    save_as_pickle(\"biases.pkl\", biases_dict)\n",
    "\n",
    "    # Calculate acc for test docs\n",
    "    batch_x = f\n",
    "    batch_y = np.eye(n_classes)[y_test]\n",
    "    test_acc = sess.run(accuracy, feed_dict = {X: batch_x,\n",
    "                                               y: batch_y,\n",
    "                                               keep_prob: 1.,\n",
    "                                               idx_selected: idx_test})\n",
    "    print(\"Testing accuracy: \", test_acc, \"Batch size: \", batch_y.shape[0])\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tfEnv]",
   "language": "python",
   "name": "conda-env-tfEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
