{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdjRD6KvpZ8K"
   },
   "source": [
    "# Drive Access\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1932,
     "status": "ok",
     "timestamp": 1583671933643,
     "user": {
      "displayName": "Dennis Triepke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj0HryPYMsO1hgMBSqhCL_XLkU10iyIfvSIDDpSOQ=s64",
      "userId": "04710653448777221570"
     },
     "user_tz": -60
    },
    "id": "uAF3_8SplOf-",
    "outputId": "3c61cee0-04b1-4786-a259-1462ec6cbad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "H8DhDQWmiu3R"
   },
   "outputs": [],
   "source": [
    "# Import PyDrive and associated libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USfkqrwJig4b"
   },
   "source": [
    "# Graph Convolutional Network\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PDPBt4Svmscm"
   },
   "outputs": [],
   "source": [
    "# Register external py module\n",
    "get_graph = drive.CreateFile({'id':'1FLj1Bm788MzPEErzDgoXUzOj5SuL-BCN'})\n",
    "get_graph.GetContentFile('get_graph.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 25144,
     "status": "ok",
     "timestamp": 1583671956929,
     "user": {
      "displayName": "Dennis Triepke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj0HryPYMsO1hgMBSqhCL_XLkU10iyIfvSIDDpSOQ=s64",
      "userId": "04710653448777221570"
     },
     "user_tz": -60
    },
    "id": "VavQPganiuh5",
    "outputId": "a26cc4f3-f622-4bdb-dddb-57cf06b363f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from get_graph import create_graph, create_graph_debug \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import pickle\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wTLlzA47iuxW"
   },
   "outputs": [],
   "source": [
    "# FILE PATH adjusted for colab\n",
    "def save_as_pickle(filename, data):\n",
    "    completeName = os.path.join(\"drive/My Drive/Coding/GCN/data/\", filename)\n",
    "    print(completeName)\n",
    "    with open(completeName, 'wb') as output:\n",
    "        pickle.dump(data, output)\n",
    "        \n",
    "def load_pickle(filename):\n",
    "    completeName = os.path.join(\"drive/My Drive/Coding/GCN/data\", filename)\n",
    "    with open(completeName, 'rb') as pkl_file:\n",
    "        data = pickle.load(pkl_file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(load_from_disc, debug, test_size = None, train_size = None, vocab_size = None):\n",
    "    \"\"\"\n",
    "    load_from_disc: load data from s3 or from own disk; the later ther must be stored priorly, \n",
    "    debug: defines if the train and test data filtered, \n",
    "    test_size: size of the test data  (Debug = True)\n",
    "    train_size = size of the train data  (Debug = True)\n",
    "    vocab_size = size of the vocabolary (load_from_disc = False)\n",
    "    \"\"\"\n",
    "    global idx_train, idx_test\n",
    "\n",
    "    if load_from_disc:\n",
    "        docs = load_pickle(\"data_joined.pkl\")\n",
    "        y_train = load_pickle(\"y_train.pkl\")\n",
    "        y_test = load_pickle(\"y_test.plk\")\n",
    "\n",
    "        # Index for semi supervised test data \n",
    "        idx_test = [i for i in range(len(y_train), len(y_train) + len(y_test) )]\n",
    "        idx_train = [i for i in range(len(y_train))]\n",
    "\n",
    "        if debug:\n",
    "            # DEBUG!!!\n",
    "            idx_train = idx_train[:train_size]\n",
    "            idx_test = idx_test[:test_size] \n",
    "\n",
    "            X_train = [docs[i] for i in idx_train]\n",
    "            X_test = [docs[i] for i in idx_test]\n",
    "            docs = np.append(X_train, X_test)\n",
    "\n",
    "            y_train = [y_train[i] for i in [idx_train]]\n",
    "            y_test = y_test[:test_size]\n",
    "\n",
    "\n",
    "    else:\n",
    "        vocab_size = vocab_size\n",
    "        imdb = keras.datasets.imdb\n",
    "\n",
    "        (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)\n",
    "\n",
    "        if debug:\n",
    "            # DEBUG!!!\n",
    "            X_train = X_train[:train_size]\n",
    "            y_train = y_train[:train_size]\n",
    "\n",
    "            X_test = X_test[:test_size]\n",
    "            y_test = y_test[:test_size]\n",
    "\n",
    "        # Decode imdb index to words\n",
    "        imdb_word_index = imdb.get_word_index()\n",
    "        imdb_word_index = {k: (v + 3) for k,v in imdb_word_index.items()}\n",
    "        imdb_word_index[\"<PAD>\"] = 0\n",
    "        imdb_word_index[\"<START>\"] = 1\n",
    "        imdb_word_index[\"<UNK>\"] = 2\n",
    "        imdb_word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "        imdb_index_word = {idx: value for value, idx in imdb_word_index.items()}\n",
    "\n",
    "        def decode_text(txt):\n",
    "            return ' '.join([imdb_index_word.get(i, '?') for i in txt])\n",
    "\n",
    "\n",
    "        # Create data for semi-supervised classifier\n",
    "        data = np.append(X_train, X_test)\n",
    "        docs = list(map(lambda i: decode_text(i), data )) # decode idx to text\n",
    "\n",
    "\n",
    "        # Index for semi supervised test data \n",
    "        idx_test = [i for i in range(len(y_train), len(y_train) + len(y_test) )]\n",
    "        idx_train = [i for i in range(len(y_train))]\n",
    "\n",
    "        # Store results\n",
    "        save_as_pickle(\"data_joined.pkl\", docs)\n",
    "        save_as_pickle(\"y_train.pkl\", y_train)\n",
    "        save_as_pickle(\"y_test.plk\", y_test)\n",
    "    \n",
    "\n",
    "    return docs, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PzHZHVaqI47"
   },
   "source": [
    "## Load Data and Create Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eoiZTMhrqXd0"
   },
   "outputs": [],
   "source": [
    "docs, y_train, y_test = load_data(load_from_disc = True, debug = True, train_size = 10000, test_size = 10000, vocab_size = 5000)\n",
    "\n",
    "# Create grapgh first time \n",
    "# G, word_word, pmi_ij = create_graph(docs)\n",
    "\n",
    "# save_as_pickle(\"text_graph.pkl\", G)\n",
    "# save_as_pickle(\"word_word.pkl\", word_word)\n",
    "# save_as_pickle(\"pmi.pkl\", pmi_ij)\n",
    "\n",
    "# Load grapgh from disk\n",
    "G = load_pickle(\"text_graph.pkl\")\n",
    "word_word = load_pickle(\"word_word.pkl\")\n",
    "pmi_ij = load_pickle(\"pmi.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6RCfZxGwDzI9"
   },
   "source": [
    "## Semi-Supervised Node Classification\n",
    "***\n",
    "\n",
    "Our model $f(X, A)$ is a two layer convolution network with sprectral convolutions. The model is flexible in terms of it relies just on the feature matrix $X = I$ and the adjacancy matrix $A$ of the graph structure.  The feature matrix $X$ is set as an identity matrix $I$, which simply means every word or document is represented as a one-hot vector.\n",
    "Regarding the author, using the adjacancy matrix is efficient in situations when $X$ possess to less information. \n",
    "\n",
    "\n",
    "**Two layer GCN:**\n",
    "\n",
    "First build a Adjacency and Degree\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{A}} = \\mathbf{D}^{1/2} \\mathbf{A} \\mathbf{D}^{1/2}\n",
    "$$\n",
    "\n",
    "and the layerwise linear model takes the form\n",
    "\n",
    "$$\n",
    "Z = f(X, A) = \\text{softmax} \\left( \\hat{A} \\text{ ReLu}(\\hat{A}X W_0) W_1  \\right)\n",
    "$$  \n",
    "\n",
    "where $W_0 \\in \\mathbb{R}^{C \\times H}$ and $W_1 \\in \\mathbb{R}^{H \\times F}$ are the network weights which are trained using gradient descent.\n",
    "$X$ is a one-hot encoding of each of the graph nodes. \n",
    "\n",
    "For the training of a semi-supervised classifier the authors using full dataset batch gradient descent and evaluate the cross-entropy just error over the labeled data: \n",
    "\n",
    "$$\n",
    "L = - \\sum_{l \\in \\mathbb{y}_L} \\sum_{f = 1}^{F} Y_{lf} ln Z_{lf}\n",
    "$$,\n",
    "\n",
    "with $\\mathbb{y}_L$ as set of document idices that have labels and $F$ is the dimension of the output features, which is equal to the number of classes.\n",
    "\n",
    "\n",
    "**Build Adjacancy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MXs5OXFPivNM"
   },
   "outputs": [],
   "source": [
    "# # Adjacancy matrix\n",
    "# A = nx.to_numpy_matrix(G, weight = \"weight\")\n",
    "# A = A + np.eye(G.number_of_nodes()) # diag = 1\n",
    "\n",
    "# # Degree matrix\n",
    "# degrees = []\n",
    "# for d in G.degree(weight = None):\n",
    "#     if d == 0:\n",
    "#         degrees.append(0)\n",
    "#     else:\n",
    "#         degrees.append(d[1]**(-0.5))\n",
    "# degrees = np.diag(degrees)\n",
    "\n",
    "# # A hat\n",
    "# A_hat = degrees@A@degrees \\\n",
    "\n",
    "# # Feature matrix\n",
    "# X = np.eye(G.number_of_nodes()) # Features are just identity matrix\n",
    "# f = X # input of the net\n",
    "\n",
    "\n",
    "# save_as_pickle(\"f.pkl\", f)\n",
    "# save_as_pickle(\"A_hat.pkl\", A_hat)\n",
    "\n",
    "f = load_pickle(\"f.pkl\")\n",
    "A_hat = load_pickle(\"A_hat.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvkQ_zP0IMd_"
   },
   "source": [
    "## Train\n",
    "\n",
    "For the trainig we need to tell the net work which nodes in the output layer (word - doc - embedding) is a labeled document node. As we have an semi supervised issue, some of the document embeddings have no label. The output matrix will have the shape of all nodes (words and documents) times the number of classes. Whereas the document indecies are the first. Hence, we just have to select the regarding output by the labeld node-indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 210083,
     "status": "ok",
     "timestamp": 1583672141932,
     "user": {
      "displayName": "Dennis Triepke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj0HryPYMsO1hgMBSqhCL_XLkU10iyIfvSIDDpSOQ=s64",
      "userId": "04710653448777221570"
     },
     "user_tz": -60
    },
    "id": "Gxc03zYKiuuw",
    "outputId": "f50d2a9c-8165-406a-af9f-69ebadc05d22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim import fully_connected\n",
    "\n",
    "# Network Parameters\n",
    "n_input = f.shape[0] # Input each node in the graph as one-hot encoding\n",
    "n_classes = 2\n",
    "dropout = 0.75\n",
    "n_hidden_1 = 330 # Size of first GCN hidden weights\n",
    "n_hidden_2 = 130\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Graph inputs\n",
    "X = tf.placeholder(tf.float32, [n_input, n_input])\n",
    "y = tf.placeholder(tf.float32,  [None, n_classes])\n",
    "\n",
    "keep_prob  = tf.placeholder(tf.float32)\n",
    "idx_selected = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "A_hat_tf = tf.convert_to_tensor(A_hat, dtype = tf.float32)\n",
    "A_hat_tf = tf.Variable(A_hat_tf)\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'h1' : tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2' : tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "}\n",
    "\n",
    "\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2' : tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "}\n",
    "\n",
    "\n",
    "def convLayer(X, A_hat_tf, w, b ):\n",
    "    X = tf.add(tf.matmul(X, w), b)  # [?,440][440, 330] + [330] = 440x330\n",
    "    X = tf.matmul(A_hat_tf, X)  # [440, 440][440,330] = [440,330]\n",
    "    \n",
    "    return tf.nn.relu(X)\n",
    "\n",
    "\n",
    "def gcn(X, weights, biases, A_hat, dropout):\n",
    "\n",
    "    # First convolution layer ?x330\n",
    "    conv1 = convLayer(X, A_hat_tf, weights['h1'], biases[\"b1\"])\n",
    "    # Second convolution layer 330x130\n",
    "    conv2 = convLayer(conv1, A_hat_tf, weights['h2'], biases[\"b2\"])\n",
    "    # Fully connected layer / Linear layer for logit\n",
    "    logits = fully_connected(conv2, n_classes, activation_fn = None)\n",
    "    # Apply Dropout\n",
    "    #logits = tf.nn.dropout(logits, dropout)\n",
    "    \n",
    "    return logits\n",
    "    \n",
    "\n",
    "# Build the GCN\n",
    "pred = gcn(X, weights, biases, A_hat_tf, dropout)\n",
    "\n",
    "#Filter training document nodes for semi-supervised learning\n",
    "pred = tf.gather(pred, indices = idx_selected)\n",
    "\n",
    "# Define optimizer and loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, axis = 1), tf.argmax(y, axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1263399,
     "status": "ok",
     "timestamp": 1583674617747,
     "user": {
      "displayName": "Dennis Triepke",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj0HryPYMsO1hgMBSqhCL_XLkU10iyIfvSIDDpSOQ=s64",
      "userId": "04710653448777221570"
     },
     "user_tz": -60
    },
    "id": "QhINn_ViIbbp",
    "outputId": "7d3a8c24-206d-49f1-d659-1da6ca0b3241"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 Batch size:  10000 Batch loss:  -25789.09 Training accuracy:  0.4947\n",
      "Epoch  1 Batch size:  10000 Batch loss:  -466873.25 Training accuracy:  0.4947\n",
      "Epoch  2 Batch size:  10000 Batch loss:  -1930127.0 Training accuracy:  0.4947\n",
      "Epoch  3 Batch size:  10000 Batch loss:  -5630913.5 Training accuracy:  0.4947\n",
      "Epoch  4 Batch size:  10000 Batch loss:  -12966361.0 Training accuracy:  0.4947\n",
      "Epoch  5 Batch size:  10000 Batch loss:  -26056398.0 Training accuracy:  0.4947\n",
      "Epoch  6 Batch size:  10000 Batch loss:  -47442228.0 Training accuracy:  0.4947\n",
      "Epoch  7 Batch size:  10000 Batch loss:  -80646424.0 Training accuracy:  0.4947\n",
      "Epoch  8 Batch size:  10000 Batch loss:  -129502340.0 Training accuracy:  0.4947\n",
      "Epoch  9 Batch size:  10000 Batch loss:  -199511330.0 Training accuracy:  0.4947\n",
      "Epoch  10 Batch size:  10000 Batch loss:  -297062600.0 Training accuracy:  0.4947\n",
      "Epoch  11 Batch size:  10000 Batch loss:  -430163550.0 Training accuracy:  0.4947\n",
      "Epoch  12 Batch size:  10000 Batch loss:  -608513300.0 Training accuracy:  0.4947\n",
      "Epoch  13 Batch size:  10000 Batch loss:  -844108400.0 Training accuracy:  0.4947\n",
      "Epoch  14 Batch size:  10000 Batch loss:  -1150821800.0 Training accuracy:  0.4947\n",
      "Epoch  15 Batch size:  10000 Batch loss:  -1546321400.0 Training accuracy:  0.4947\n",
      "Epoch  16 Batch size:  10000 Batch loss:  -2050903700.0 Training accuracy:  0.4947\n",
      "Epoch  17 Batch size:  10000 Batch loss:  -2688888000.0 Training accuracy:  0.4947\n",
      "Epoch  18 Batch size:  10000 Batch loss:  -3489074000.0 Training accuracy:  0.4947\n",
      "Epoch  19 Batch size:  10000 Batch loss:  -4485275600.0 Training accuracy:  0.4947\n",
      "Epoch  20 Batch size:  10000 Batch loss:  -5717039000.0 Training accuracy:  0.4947\n",
      "Epoch  21 Batch size:  10000 Batch loss:  -7230040600.0 Training accuracy:  1.0\n",
      "Epoch  22 Batch size:  10000 Batch loss:  -9077774000.0 Training accuracy:  0.9999\n",
      "Epoch  23 Batch size:  10000 Batch loss:  -11321501000.0 Training accuracy:  0.9946\n",
      "Epoch  24 Batch size:  10000 Batch loss:  -14032596000.0 Training accuracy:  0.979\n",
      "Epoch  25 Batch size:  10000 Batch loss:  -17292468000.0 Training accuracy:  0.9475\n",
      "Epoch  26 Batch size:  10000 Batch loss:  -21194213000.0 Training accuracy:  0.8737\n",
      "Epoch  27 Batch size:  10000 Batch loss:  -25844027000.0 Training accuracy:  0.7187\n",
      "Epoch  28 Batch size:  10000 Batch loss:  -31361855000.0 Training accuracy:  0.5917\n",
      "Epoch  29 Batch size:  10000 Batch loss:  -37884076000.0 Training accuracy:  0.5334\n",
      "Epoch  30 Batch size:  10000 Batch loss:  -45564600000.0 Training accuracy:  0.5334\n",
      "Epoch  31 Batch size:  10000 Batch loss:  -54576673000.0 Training accuracy:  0.5334\n",
      "Epoch  32 Batch size:  10000 Batch loss:  -65114632000.0 Training accuracy:  0.5334\n",
      "Epoch  33 Batch size:  10000 Batch loss:  -77395890000.0 Training accuracy:  0.5334\n",
      "Epoch  34 Batch size:  10000 Batch loss:  -91663140000.0 Training accuracy:  0.5334\n",
      "Epoch  35 Batch size:  10000 Batch loss:  -108186630000.0 Training accuracy:  0.5334\n",
      "Epoch  36 Batch size:  10000 Batch loss:  -127266600000.0 Training accuracy:  0.5334\n",
      "Epoch  37 Batch size:  10000 Batch loss:  -149235920000.0 Training accuracy:  0.5334\n",
      "Epoch  38 Batch size:  10000 Batch loss:  -174462730000.0 Training accuracy:  0.5334\n",
      "Epoch  39 Batch size:  10000 Batch loss:  -203353310000.0 Training accuracy:  0.5334\n",
      "Epoch  40 Batch size:  10000 Batch loss:  -236354730000.0 Training accuracy:  0.5334\n",
      "Epoch  41 Batch size:  10000 Batch loss:  -273960240000.0 Training accuracy:  0.5334\n",
      "Epoch  42 Batch size:  10000 Batch loss:  -316708260000.0 Training accuracy:  0.5334\n",
      "Epoch  43 Batch size:  10000 Batch loss:  -365190000000.0 Training accuracy:  0.5334\n",
      "Epoch  44 Batch size:  10000 Batch loss:  -420051300000.0 Training accuracy:  0.5334\n",
      "Epoch  45 Batch size:  10000 Batch loss:  -481996870000.0 Training accuracy:  0.5334\n",
      "Epoch  46 Batch size:  10000 Batch loss:  -551794000000.0 Training accuracy:  0.5334\n",
      "Epoch  47 Batch size:  10000 Batch loss:  -630277340000.0 Training accuracy:  0.5334\n",
      "Epoch  48 Batch size:  10000 Batch loss:  -718352600000.0 Training accuracy:  0.5334\n",
      "Epoch  49 Batch size:  10000 Batch loss:  -817001460000.0 Training accuracy:  0.5334\n",
      "Epoch  50 Batch size:  10000 Batch loss:  -927286560000.0 Training accuracy:  0.5334\n",
      "Epoch  51 Batch size:  10000 Batch loss:  -1050355600000.0 Training accuracy:  0.5334\n",
      "Epoch  52 Batch size:  10000 Batch loss:  -1187447700000.0 Training accuracy:  0.5334\n",
      "Epoch  53 Batch size:  10000 Batch loss:  -1339897300000.0 Training accuracy:  0.5334\n",
      "Epoch  54 Batch size:  10000 Batch loss:  -1509140700000.0 Training accuracy:  0.5334\n",
      "Epoch  55 Batch size:  10000 Batch loss:  -1696721300000.0 Training accuracy:  0.5334\n",
      "Epoch  56 Batch size:  10000 Batch loss:  -1904294900000.0 Training accuracy:  0.5334\n",
      "Epoch  57 Batch size:  10000 Batch loss:  -2133636700000.0 Training accuracy:  0.5334\n",
      "Epoch  58 Batch size:  10000 Batch loss:  -2386647000000.0 Training accuracy:  0.5334\n",
      "Epoch  59 Batch size:  10000 Batch loss:  -2665358000000.0 Training accuracy:  0.5334\n",
      "Epoch  60 Batch size:  10000 Batch loss:  -2971940000000.0 Training accuracy:  0.5334\n",
      "Epoch  61 Batch size:  10000 Batch loss:  -3308708000000.0 Training accuracy:  0.5334\n",
      "Epoch  62 Batch size:  10000 Batch loss:  -3678128300000.0 Training accuracy:  0.5334\n",
      "Epoch  63 Batch size:  10000 Batch loss:  -4082826700000.0 Training accuracy:  0.5334\n",
      "Epoch  64 Batch size:  10000 Batch loss:  -4525597000000.0 Training accuracy:  0.5334\n",
      "Epoch  65 Batch size:  10000 Batch loss:  -5009403500000.0 Training accuracy:  0.5334\n",
      "Epoch  66 Batch size:  10000 Batch loss:  -5537396000000.0 Training accuracy:  0.5334\n",
      "Epoch  67 Batch size:  10000 Batch loss:  -6112911000000.0 Training accuracy:  0.5334\n",
      "Epoch  68 Batch size:  10000 Batch loss:  -6739482000000.0 Training accuracy:  0.5334\n",
      "Epoch  69 Batch size:  10000 Batch loss:  -7420850500000.0 Training accuracy:  0.5334\n",
      "Epoch  70 Batch size:  10000 Batch loss:  -8160971000000.0 Training accuracy:  0.5334\n",
      "Epoch  71 Batch size:  10000 Batch loss:  -8964021000000.0 Training accuracy:  0.5334\n",
      "Epoch  72 Batch size:  10000 Batch loss:  -9834410000000.0 Training accuracy:  0.5334\n",
      "Epoch  73 Batch size:  10000 Batch loss:  -10776785000000.0 Training accuracy:  0.5334\n",
      "Epoch  74 Batch size:  10000 Batch loss:  -11796047000000.0 Training accuracy:  0.5334\n",
      "Epoch  75 Batch size:  10000 Batch loss:  -12897352000000.0 Training accuracy:  0.5334\n",
      "Epoch  76 Batch size:  10000 Batch loss:  -14086126000000.0 Training accuracy:  0.5334\n",
      "Epoch  77 Batch size:  10000 Batch loss:  -15368075000000.0 Training accuracy:  0.5334\n",
      "Epoch  78 Batch size:  10000 Batch loss:  -16749188000000.0 Training accuracy:  0.5334\n",
      "Epoch  79 Batch size:  10000 Batch loss:  -18235752000000.0 Training accuracy:  0.5334\n",
      "Epoch  80 Batch size:  10000 Batch loss:  -19834364000000.0 Training accuracy:  0.5334\n",
      "Epoch  81 Batch size:  10000 Batch loss:  -21551942000000.0 Training accuracy:  0.5334\n",
      "Epoch  82 Batch size:  10000 Batch loss:  -23395723000000.0 Training accuracy:  0.5334\n",
      "Epoch  83 Batch size:  10000 Batch loss:  -25373293000000.0 Training accuracy:  0.5334\n",
      "Epoch  84 Batch size:  10000 Batch loss:  -27492576000000.0 Training accuracy:  0.5334\n",
      "Epoch  85 Batch size:  10000 Batch loss:  -29761869000000.0 Training accuracy:  0.5334\n",
      "Epoch  86 Batch size:  10000 Batch loss:  -32189832000000.0 Training accuracy:  0.5334\n",
      "Epoch  87 Batch size:  10000 Batch loss:  -34785506000000.0 Training accuracy:  0.5334\n",
      "Epoch  88 Batch size:  10000 Batch loss:  -37558327000000.0 Training accuracy:  0.5334\n",
      "Epoch  89 Batch size:  10000 Batch loss:  -40518150000000.0 Training accuracy:  0.5334\n",
      "Epoch  90 Batch size:  10000 Batch loss:  -43675220000000.0 Training accuracy:  0.5334\n",
      "Epoch  91 Batch size:  10000 Batch loss:  -47040230000000.0 Training accuracy:  0.5334\n",
      "Epoch  92 Batch size:  10000 Batch loss:  -50624314000000.0 Training accuracy:  0.5334\n",
      "Epoch  93 Batch size:  10000 Batch loss:  -54439038000000.0 Training accuracy:  0.5334\n",
      "Epoch  94 Batch size:  10000 Batch loss:  -58496444000000.0 Training accuracy:  0.5334\n",
      "Epoch  95 Batch size:  10000 Batch loss:  -62809065000000.0 Training accuracy:  0.5334\n",
      "Epoch  96 Batch size:  10000 Batch loss:  -67389890000000.0 Training accuracy:  0.5334\n",
      "Epoch  97 Batch size:  10000 Batch loss:  -72252440000000.0 Training accuracy:  0.5334\n",
      "Epoch  98 Batch size:  10000 Batch loss:  -77410720000000.0 Training accuracy:  0.5334\n",
      "Epoch  99 Batch size:  10000 Batch loss:  -82879304000000.0 Training accuracy:  0.5334\n",
      "Epoch  100 Batch size:  10000 Batch loss:  -88673240000000.0 Training accuracy:  0.5334\n",
      "Epoch  101 Batch size:  10000 Batch loss:  -94808190000000.0 Training accuracy:  0.5334\n",
      "Epoch  102 Batch size:  10000 Batch loss:  -101300330000000.0 Training accuracy:  0.5334\n",
      "Epoch  103 Batch size:  10000 Batch loss:  -108166450000000.0 Training accuracy:  0.5334\n",
      "Epoch  104 Batch size:  10000 Batch loss:  -115423900000000.0 Training accuracy:  0.5334\n",
      "Epoch  105 Batch size:  10000 Batch loss:  -123090660000000.0 Training accuracy:  0.5334\n",
      "Epoch  106 Batch size:  10000 Batch loss:  -131185310000000.0 Training accuracy:  0.5334\n",
      "Epoch  107 Batch size:  10000 Batch loss:  -139727060000000.0 Training accuracy:  0.5334\n",
      "Epoch  108 Batch size:  10000 Batch loss:  -148735800000000.0 Training accuracy:  0.5334\n",
      "Epoch  109 Batch size:  10000 Batch loss:  -158231960000000.0 Training accuracy:  0.5334\n",
      "Epoch  110 Batch size:  10000 Batch loss:  -168236800000000.0 Training accuracy:  0.5334\n",
      "Epoch  111 Batch size:  10000 Batch loss:  -178772180000000.0 Training accuracy:  0.5334\n",
      "Epoch  112 Batch size:  10000 Batch loss:  -189860620000000.0 Training accuracy:  0.5334\n",
      "Epoch  113 Batch size:  10000 Batch loss:  -201525420000000.0 Training accuracy:  0.5334\n",
      "Epoch  114 Batch size:  10000 Batch loss:  -213790590000000.0 Training accuracy:  0.5334\n",
      "Epoch  115 Batch size:  10000 Batch loss:  -226680800000000.0 Training accuracy:  0.5334\n",
      "Epoch  116 Batch size:  10000 Batch loss:  -240221630000000.0 Training accuracy:  0.5334\n",
      "Epoch  117 Batch size:  10000 Batch loss:  -254439300000000.0 Training accuracy:  0.5334\n",
      "Epoch  118 Batch size:  10000 Batch loss:  -269360840000000.0 Training accuracy:  0.5334\n",
      "Epoch  119 Batch size:  10000 Batch loss:  -285014000000000.0 Training accuracy:  0.5334\n",
      "Epoch  120 Batch size:  10000 Batch loss:  -301427580000000.0 Training accuracy:  0.5334\n",
      "Epoch  121 Batch size:  10000 Batch loss:  -318630970000000.0 Training accuracy:  0.5334\n",
      "Epoch  122 Batch size:  10000 Batch loss:  -336654500000000.0 Training accuracy:  0.5334\n",
      "Epoch  123 Batch size:  10000 Batch loss:  -355529170000000.0 Training accuracy:  0.5334\n",
      "Epoch  124 Batch size:  10000 Batch loss:  -375287330000000.0 Training accuracy:  0.5334\n",
      "Epoch  125 Batch size:  10000 Batch loss:  -395961400000000.0 Training accuracy:  0.5334\n",
      "Epoch  126 Batch size:  10000 Batch loss:  -417585600000000.0 Training accuracy:  0.5334\n",
      "Epoch  127 Batch size:  10000 Batch loss:  -440194420000000.0 Training accuracy:  0.5334\n",
      "Epoch  128 Batch size:  10000 Batch loss:  -463823550000000.0 Training accuracy:  0.5334\n",
      "Epoch  129 Batch size:  10000 Batch loss:  -488509500000000.0 Training accuracy:  0.5334\n",
      "Epoch  130 Batch size:  10000 Batch loss:  -514289850000000.0 Training accuracy:  0.5334\n",
      "Epoch  131 Batch size:  10000 Batch loss:  -541203060000000.0 Training accuracy:  0.5334\n",
      "Epoch  132 Batch size:  10000 Batch loss:  -569288650000000.0 Training accuracy:  0.5334\n",
      "Epoch  133 Batch size:  10000 Batch loss:  -598586900000000.0 Training accuracy:  0.5334\n",
      "Epoch  134 Batch size:  10000 Batch loss:  -629139400000000.0 Training accuracy:  0.5334\n",
      "Epoch  135 Batch size:  10000 Batch loss:  -660988800000000.0 Training accuracy:  0.5334\n",
      "Epoch  136 Batch size:  10000 Batch loss:  -694178200000000.0 Training accuracy:  0.5334\n",
      "Epoch  137 Batch size:  10000 Batch loss:  -728753000000000.0 Training accuracy:  0.5334\n",
      "Epoch  138 Batch size:  10000 Batch loss:  -764758100000000.0 Training accuracy:  0.5334\n",
      "Epoch  139 Batch size:  10000 Batch loss:  -802240370000000.0 Training accuracy:  0.5334\n",
      "Epoch  140 Batch size:  10000 Batch loss:  -841247600000000.0 Training accuracy:  0.5334\n",
      "Epoch  141 Batch size:  10000 Batch loss:  -881829600000000.0 Training accuracy:  0.5334\n",
      "Epoch  142 Batch size:  10000 Batch loss:  -924035300000000.0 Training accuracy:  0.5334\n",
      "Epoch  143 Batch size:  10000 Batch loss:  -967916450000000.0 Training accuracy:  0.5334\n",
      "Epoch  144 Batch size:  10000 Batch loss:  -1013525850000000.0 Training accuracy:  0.5334\n",
      "Epoch  145 Batch size:  10000 Batch loss:  -1060916450000000.0 Training accuracy:  0.5334\n",
      "Epoch  146 Batch size:  10000 Batch loss:  -1110143150000000.0 Training accuracy:  0.5334\n",
      "Epoch  147 Batch size:  10000 Batch loss:  -1161261700000000.0 Training accuracy:  0.5334\n",
      "Epoch  148 Batch size:  10000 Batch loss:  -1214329400000000.0 Training accuracy:  0.5334\n",
      "Epoch  149 Batch size:  10000 Batch loss:  -1269405400000000.0 Training accuracy:  0.5334\n",
      "Epoch  150 Batch size:  10000 Batch loss:  -1326548300000000.0 Training accuracy:  0.5334\n",
      "Epoch  151 Batch size:  10000 Batch loss:  -1385819400000000.0 Training accuracy:  0.5334\n",
      "Epoch  152 Batch size:  10000 Batch loss:  -1447281200000000.0 Training accuracy:  0.5334\n",
      "Epoch  153 Batch size:  10000 Batch loss:  -1510996900000000.0 Training accuracy:  0.5334\n",
      "Epoch  154 Batch size:  10000 Batch loss:  -1577031300000000.0 Training accuracy:  0.5334\n",
      "Epoch  155 Batch size:  10000 Batch loss:  -1645450200000000.0 Training accuracy:  0.5334\n",
      "Epoch  156 Batch size:  10000 Batch loss:  -1716321100000000.0 Training accuracy:  0.5334\n",
      "Epoch  157 Batch size:  10000 Batch loss:  -1789713700000000.0 Training accuracy:  0.5334\n",
      "Epoch  158 Batch size:  10000 Batch loss:  -1865697000000000.0 Training accuracy:  0.5334\n",
      "Epoch  159 Batch size:  10000 Batch loss:  -1944343800000000.0 Training accuracy:  0.5334\n",
      "Epoch  160 Batch size:  10000 Batch loss:  -2025725200000000.0 Training accuracy:  0.5334\n",
      "Epoch  161 Batch size:  10000 Batch loss:  -2109916600000000.0 Training accuracy:  0.5334\n",
      "Epoch  162 Batch size:  10000 Batch loss:  -2196994400000000.0 Training accuracy:  0.5334\n",
      "Epoch  163 Batch size:  10000 Batch loss:  -2287036000000000.0 Training accuracy:  0.5334\n",
      "Epoch  164 Batch size:  10000 Batch loss:  -2380117300000000.0 Training accuracy:  0.5334\n",
      "Epoch  165 Batch size:  10000 Batch loss:  -2476320000000000.0 Training accuracy:  0.5334\n",
      "Epoch  166 Batch size:  10000 Batch loss:  -2575727000000000.0 Training accuracy:  0.5334\n",
      "Epoch  167 Batch size:  10000 Batch loss:  -2678420000000000.0 Training accuracy:  0.5334\n",
      "Epoch  168 Batch size:  10000 Batch loss:  -2784484700000000.0 Training accuracy:  0.5334\n",
      "Epoch  169 Batch size:  10000 Batch loss:  -2894005600000000.0 Training accuracy:  0.5334\n",
      "Epoch  170 Batch size:  10000 Batch loss:  -3007071200000000.0 Training accuracy:  0.5334\n",
      "Epoch  171 Batch size:  10000 Batch loss:  -3123771600000000.0 Training accuracy:  0.5334\n",
      "Epoch  172 Batch size:  10000 Batch loss:  -3244195200000000.0 Training accuracy:  0.5334\n",
      "Epoch  173 Batch size:  10000 Batch loss:  -3368437000000000.0 Training accuracy:  0.5334\n",
      "Epoch  174 Batch size:  10000 Batch loss:  -3496589500000000.0 Training accuracy:  0.5334\n",
      "Epoch  175 Batch size:  10000 Batch loss:  -3628749000000000.0 Training accuracy:  0.5334\n",
      "Epoch  176 Batch size:  10000 Batch loss:  -3765013200000000.0 Training accuracy:  0.5334\n",
      "Epoch  177 Batch size:  10000 Batch loss:  -3905478500000000.0 Training accuracy:  0.5334\n",
      "Epoch  178 Batch size:  10000 Batch loss:  -4050247300000000.0 Training accuracy:  0.5334\n",
      "Epoch  179 Batch size:  10000 Batch loss:  -4199420000000000.0 Training accuracy:  0.5334\n",
      "Epoch  180 Batch size:  10000 Batch loss:  -4353103700000000.0 Training accuracy:  0.5334\n",
      "Epoch  181 Batch size:  10000 Batch loss:  -4511400000000000.0 Training accuracy:  0.5334\n",
      "Epoch  182 Batch size:  10000 Batch loss:  -4674418500000000.0 Training accuracy:  0.5334\n",
      "Epoch  183 Batch size:  10000 Batch loss:  -4842267000000000.0 Training accuracy:  0.5334\n",
      "Epoch  184 Batch size:  10000 Batch loss:  -5015062000000000.0 Training accuracy:  0.5334\n",
      "Epoch  185 Batch size:  10000 Batch loss:  -5192900000000000.0 Training accuracy:  0.5334\n",
      "Epoch  186 Batch size:  10000 Batch loss:  -5375920000000000.0 Training accuracy:  0.5334\n",
      "Epoch  187 Batch size:  10000 Batch loss:  -5564216600000000.0 Training accuracy:  0.5334\n",
      "Epoch  188 Batch size:  10000 Batch loss:  -5757914000000000.0 Training accuracy:  0.5334\n",
      "Epoch  189 Batch size:  10000 Batch loss:  -5957133000000000.0 Training accuracy:  0.5334\n",
      "Epoch  190 Batch size:  10000 Batch loss:  -6161996000000000.0 Training accuracy:  0.5334\n",
      "Epoch  191 Batch size:  10000 Batch loss:  -6372625500000000.0 Training accuracy:  0.5334\n",
      "Epoch  192 Batch size:  10000 Batch loss:  -6589146000000000.0 Training accuracy:  0.5334\n",
      "Epoch  193 Batch size:  10000 Batch loss:  -6811683400000000.0 Training accuracy:  0.5334\n",
      "Epoch  194 Batch size:  10000 Batch loss:  -7040373000000000.0 Training accuracy:  0.5334\n",
      "Epoch  195 Batch size:  10000 Batch loss:  -7275331000000000.0 Training accuracy:  0.5334\n",
      "Epoch  196 Batch size:  10000 Batch loss:  -7516705000000000.0 Training accuracy:  0.5334\n",
      "Epoch  197 Batch size:  10000 Batch loss:  -7764623000000000.0 Training accuracy:  0.5334\n",
      "Epoch  198 Batch size:  10000 Batch loss:  -8019220400000000.0 Training accuracy:  0.5334\n",
      "Epoch  199 Batch size:  10000 Batch loss:  -8280642000000000.0 Training accuracy:  0.5334\n",
      "Epoch  200 Batch size:  10000 Batch loss:  -8549019700000000.0 Training accuracy:  0.5334\n",
      "Epoch  201 Batch size:  10000 Batch loss:  -8824497000000000.0 Training accuracy:  0.5334\n",
      "Epoch  202 Batch size:  10000 Batch loss:  -9107226000000000.0 Training accuracy:  0.5334\n",
      "Epoch  203 Batch size:  10000 Batch loss:  -9397353000000000.0 Training accuracy:  0.5334\n",
      "Epoch  204 Batch size:  10000 Batch loss:  -9695006000000000.0 Training accuracy:  0.5334\n",
      "Epoch  205 Batch size:  10000 Batch loss:  -1.0000348e+16 Training accuracy:  0.5334\n",
      "Epoch  206 Batch size:  10000 Batch loss:  -1.0313557e+16 Training accuracy:  0.5334\n",
      "Epoch  207 Batch size:  10000 Batch loss:  -1.063474e+16 Training accuracy:  0.5334\n",
      "Epoch  208 Batch size:  10000 Batch loss:  -1.0964104e+16 Training accuracy:  0.5334\n",
      "Epoch  209 Batch size:  10000 Batch loss:  -1.130175e+16 Training accuracy:  0.5334\n",
      "Epoch  210 Batch size:  10000 Batch loss:  -1.1647901e+16 Training accuracy:  0.5334\n",
      "Epoch  211 Batch size:  10000 Batch loss:  -1.2002654e+16 Training accuracy:  0.5334\n",
      "Epoch  212 Batch size:  10000 Batch loss:  -1.2366235e+16 Training accuracy:  0.5334\n",
      "Epoch  213 Batch size:  10000 Batch loss:  -1.2738773e+16 Training accuracy:  0.5334\n",
      "Epoch  214 Batch size:  10000 Batch loss:  -1.312044e+16 Training accuracy:  0.5334\n",
      "Epoch  215 Batch size:  10000 Batch loss:  -1.3511445e+16 Training accuracy:  0.5334\n",
      "Epoch  216 Batch size:  10000 Batch loss:  -1.3911914e+16 Training accuracy:  0.5334\n",
      "Epoch  217 Batch size:  10000 Batch loss:  -1.4322029e+16 Training accuracy:  0.5334\n",
      "Epoch  218 Batch size:  10000 Batch loss:  -1.4741996e+16 Training accuracy:  0.5334\n",
      "Epoch  219 Batch size:  10000 Batch loss:  -1.5171957e+16 Training accuracy:  0.5334\n",
      "Epoch  220 Batch size:  10000 Batch loss:  -1.5612134e+16 Training accuracy:  0.5334\n",
      "Epoch  221 Batch size:  10000 Batch loss:  -1.6062693e+16 Training accuracy:  0.5334\n",
      "Epoch  222 Batch size:  10000 Batch loss:  -1.6523818e+16 Training accuracy:  0.5334\n",
      "Epoch  223 Batch size:  10000 Batch loss:  -1.69957e+16 Training accuracy:  0.5334\n",
      "Epoch  224 Batch size:  10000 Batch loss:  -1.747852e+16 Training accuracy:  0.5334\n",
      "Epoch  225 Batch size:  10000 Batch loss:  -1.7972483e+16 Training accuracy:  0.5334\n",
      "Epoch  226 Batch size:  10000 Batch loss:  -1.8477793e+16 Training accuracy:  0.5334\n",
      "Epoch  227 Batch size:  10000 Batch loss:  -1.8994635e+16 Training accuracy:  0.5334\n",
      "Epoch  228 Batch size:  10000 Batch loss:  -1.9523214e+16 Training accuracy:  0.5334\n",
      "Epoch  229 Batch size:  10000 Batch loss:  -2.0063731e+16 Training accuracy:  0.5334\n",
      "Epoch  230 Batch size:  10000 Batch loss:  -2.0616397e+16 Training accuracy:  0.5334\n",
      "Epoch  231 Batch size:  10000 Batch loss:  -2.11814e+16 Training accuracy:  0.5334\n",
      "Epoch  232 Batch size:  10000 Batch loss:  -2.1758968e+16 Training accuracy:  0.5334\n",
      "Epoch  233 Batch size:  10000 Batch loss:  -2.2349311e+16 Training accuracy:  0.5334\n",
      "Epoch  234 Batch size:  10000 Batch loss:  -2.2952649e+16 Training accuracy:  0.5334\n",
      "Epoch  235 Batch size:  10000 Batch loss:  -2.3569166e+16 Training accuracy:  0.5334\n",
      "Epoch  236 Batch size:  10000 Batch loss:  -2.419912e+16 Training accuracy:  0.5334\n",
      "Epoch  237 Batch size:  10000 Batch loss:  -2.484273e+16 Training accuracy:  0.5334\n",
      "Epoch  238 Batch size:  10000 Batch loss:  -2.5500198e+16 Training accuracy:  0.5334\n",
      "Epoch  239 Batch size:  10000 Batch loss:  -2.6171703e+16 Training accuracy:  0.5334\n",
      "Epoch  240 Batch size:  10000 Batch loss:  -2.6857607e+16 Training accuracy:  0.5334\n",
      "Epoch  241 Batch size:  10000 Batch loss:  -2.7558028e+16 Training accuracy:  0.5334\n",
      "Epoch  242 Batch size:  10000 Batch loss:  -2.82733e+16 Training accuracy:  0.5334\n",
      "Epoch  243 Batch size:  10000 Batch loss:  -2.9003536e+16 Training accuracy:  0.5334\n",
      "Epoch  244 Batch size:  10000 Batch loss:  -2.9749044e+16 Training accuracy:  0.5334\n",
      "Epoch  245 Batch size:  10000 Batch loss:  -3.051013e+16 Training accuracy:  0.5334\n",
      "Epoch  246 Batch size:  10000 Batch loss:  -3.1286816e+16 Training accuracy:  0.5334\n",
      "Epoch  247 Batch size:  10000 Batch loss:  -3.2079633e+16 Training accuracy:  0.5334\n",
      "Epoch  248 Batch size:  10000 Batch loss:  -3.2888673e+16 Training accuracy:  0.5334\n",
      "Epoch  249 Batch size:  10000 Batch loss:  -3.371414e+16 Training accuracy:  0.5334\n",
      "Epoch  250 Batch size:  10000 Batch loss:  -3.4556426e+16 Training accuracy:  0.5334\n",
      "Epoch  251 Batch size:  10000 Batch loss:  -3.5415667e+16 Training accuracy:  0.5334\n",
      "Epoch  252 Batch size:  10000 Batch loss:  -3.629219e+16 Training accuracy:  0.5334\n",
      "Epoch  253 Batch size:  10000 Batch loss:  -3.7186286e+16 Training accuracy:  0.5334\n",
      "Epoch  254 Batch size:  10000 Batch loss:  -3.8098142e+16 Training accuracy:  0.5334\n",
      "Epoch  255 Batch size:  10000 Batch loss:  -3.9028054e+16 Training accuracy:  0.5334\n",
      "Epoch  256 Batch size:  10000 Batch loss:  -3.9976276e+16 Training accuracy:  0.5334\n",
      "Epoch  257 Batch size:  10000 Batch loss:  -4.094309e+16 Training accuracy:  0.5334\n",
      "Epoch  258 Batch size:  10000 Batch loss:  -4.192883e+16 Training accuracy:  0.5334\n",
      "Epoch  259 Batch size:  10000 Batch loss:  -4.29337e+16 Training accuracy:  0.5334\n",
      "Epoch  260 Batch size:  10000 Batch loss:  -4.3958002e+16 Training accuracy:  0.5334\n",
      "Epoch  261 Batch size:  10000 Batch loss:  -4.500203e+16 Training accuracy:  0.5334\n",
      "Epoch  262 Batch size:  10000 Batch loss:  -4.6066054e+16 Training accuracy:  0.5334\n",
      "Epoch  263 Batch size:  10000 Batch loss:  -4.7150396e+16 Training accuracy:  0.5334\n",
      "Epoch  264 Batch size:  10000 Batch loss:  -4.825526e+16 Training accuracy:  0.5334\n",
      "Epoch  265 Batch size:  10000 Batch loss:  -4.9381056e+16 Training accuracy:  0.5334\n",
      "Epoch  266 Batch size:  10000 Batch loss:  -5.052805e+16 Training accuracy:  0.5334\n",
      "Epoch  267 Batch size:  10000 Batch loss:  -5.1696262e+16 Training accuracy:  0.5334\n",
      "Epoch  268 Batch size:  10000 Batch loss:  -5.288668e+16 Training accuracy:  0.5334\n",
      "Epoch  269 Batch size:  10000 Batch loss:  -5.409886e+16 Training accuracy:  0.5334\n",
      "Epoch  270 Batch size:  10000 Batch loss:  -5.533347e+16 Training accuracy:  0.5334\n",
      "Epoch  271 Batch size:  10000 Batch loss:  -5.6590794e+16 Training accuracy:  0.5334\n",
      "Epoch  272 Batch size:  10000 Batch loss:  -5.7871205e+16 Training accuracy:  0.5334\n",
      "Epoch  273 Batch size:  10000 Batch loss:  -5.917492e+16 Training accuracy:  0.5334\n",
      "Epoch  274 Batch size:  10000 Batch loss:  -6.050226e+16 Training accuracy:  0.5334\n",
      "Epoch  275 Batch size:  10000 Batch loss:  -6.1853533e+16 Training accuracy:  0.5334\n",
      "Epoch  276 Batch size:  10000 Batch loss:  -6.3229203e+16 Training accuracy:  0.5334\n",
      "Epoch  277 Batch size:  10000 Batch loss:  -6.462937e+16 Training accuracy:  0.5334\n",
      "Epoch  278 Batch size:  10000 Batch loss:  -6.60545e+16 Training accuracy:  0.5334\n",
      "Epoch  279 Batch size:  10000 Batch loss:  -6.750478e+16 Training accuracy:  0.5334\n",
      "Epoch  280 Batch size:  10000 Batch loss:  -6.8981036e+16 Training accuracy:  0.5334\n",
      "Epoch  281 Batch size:  10000 Batch loss:  -7.0482827e+16 Training accuracy:  0.5334\n",
      "Epoch  282 Batch size:  10000 Batch loss:  -7.2011264e+16 Training accuracy:  0.5334\n",
      "Epoch  283 Batch size:  10000 Batch loss:  -7.356622e+16 Training accuracy:  0.5334\n",
      "Epoch  284 Batch size:  10000 Batch loss:  -7.514809e+16 Training accuracy:  0.5334\n",
      "Epoch  285 Batch size:  10000 Batch loss:  -7.675734e+16 Training accuracy:  0.5334\n",
      "Epoch  286 Batch size:  10000 Batch loss:  -7.83943e+16 Training accuracy:  0.5334\n",
      "Epoch  287 Batch size:  10000 Batch loss:  -8.005937e+16 Training accuracy:  0.5334\n",
      "Epoch  288 Batch size:  10000 Batch loss:  -8.175269e+16 Training accuracy:  0.5334\n",
      "Epoch  289 Batch size:  10000 Batch loss:  -8.34749e+16 Training accuracy:  0.5334\n",
      "Epoch  290 Batch size:  10000 Batch loss:  -8.522619e+16 Training accuracy:  0.5334\n",
      "Epoch  291 Batch size:  10000 Batch loss:  -8.7007155e+16 Training accuracy:  0.5334\n",
      "Epoch  292 Batch size:  10000 Batch loss:  -8.881769e+16 Training accuracy:  0.5334\n",
      "Epoch  293 Batch size:  10000 Batch loss:  -9.065866e+16 Training accuracy:  0.5334\n",
      "Epoch  294 Batch size:  10000 Batch loss:  -9.253012e+16 Training accuracy:  0.5334\n",
      "Epoch  295 Batch size:  10000 Batch loss:  -9.443277e+16 Training accuracy:  0.5334\n",
      "Epoch  296 Batch size:  10000 Batch loss:  -9.636673e+16 Training accuracy:  0.5334\n",
      "Epoch  297 Batch size:  10000 Batch loss:  -9.83324e+16 Training accuracy:  0.5334\n",
      "Epoch  298 Batch size:  10000 Batch loss:  -1.0033024e+17 Training accuracy:  0.5334\n",
      "Epoch  299 Batch size:  10000 Batch loss:  -1.0236073e+17 Training accuracy:  0.5334\n",
      "Epoch  300 Batch size:  10000 Batch loss:  -1.0442411e+17 Training accuracy:  0.5334\n",
      "Epoch  301 Batch size:  10000 Batch loss:  -1.0652096e+17 Training accuracy:  0.5334\n",
      "Epoch  302 Batch size:  10000 Batch loss:  -1.0865128e+17 Training accuracy:  0.5334\n",
      "Epoch  303 Batch size:  10000 Batch loss:  -1.1081595e+17 Training accuracy:  0.5334\n",
      "Epoch  304 Batch size:  10000 Batch loss:  -1.1301512e+17 Training accuracy:  0.5334\n",
      "Epoch  305 Batch size:  10000 Batch loss:  -1.1524939e+17 Training accuracy:  0.5334\n",
      "Epoch  306 Batch size:  10000 Batch loss:  -1.1751904e+17 Training accuracy:  0.5334\n",
      "Epoch  307 Batch size:  10000 Batch loss:  -1.1982425e+17 Training accuracy:  0.5334\n",
      "Epoch  308 Batch size:  10000 Batch loss:  -1.2216594e+17 Training accuracy:  0.5334\n",
      "Epoch  309 Batch size:  10000 Batch loss:  -1.2454404e+17 Training accuracy:  0.5334\n",
      "Epoch  310 Batch size:  10000 Batch loss:  -1.26959405e+17 Training accuracy:  0.5334\n",
      "Epoch  311 Batch size:  10000 Batch loss:  -1.2941198e+17 Training accuracy:  0.5334\n",
      "Epoch  312 Batch size:  10000 Batch loss:  -1.3190268e+17 Training accuracy:  0.5334\n",
      "Epoch  313 Batch size:  10000 Batch loss:  -1.344318e+17 Training accuracy:  0.5334\n",
      "Epoch  314 Batch size:  10000 Batch loss:  -1.3699967e+17 Training accuracy:  0.5334\n",
      "Epoch  315 Batch size:  10000 Batch loss:  -1.3960716e+17 Training accuracy:  0.5334\n",
      "Epoch  316 Batch size:  10000 Batch loss:  -1.4225432e+17 Training accuracy:  0.5334\n",
      "Epoch  317 Batch size:  10000 Batch loss:  -1.4494065e+17 Training accuracy:  0.5334\n",
      "Epoch  318 Batch size:  10000 Batch loss:  -1.4766852e+17 Training accuracy:  0.5334\n",
      "Epoch  319 Batch size:  10000 Batch loss:  -1.5043709e+17 Training accuracy:  0.5334\n",
      "Epoch  320 Batch size:  10000 Batch loss:  -1.5324734e+17 Training accuracy:  0.5334\n",
      "Epoch  321 Batch size:  10000 Batch loss:  -1.5609932e+17 Training accuracy:  0.5334\n",
      "Epoch  322 Batch size:  10000 Batch loss:  -1.5899395e+17 Training accuracy:  0.5334\n",
      "Epoch  323 Batch size:  10000 Batch loss:  -1.6193216e+17 Training accuracy:  0.5334\n",
      "Epoch  324 Batch size:  10000 Batch loss:  -1.6491154e+17 Training accuracy:  0.5334\n",
      "Epoch  325 Batch size:  10000 Batch loss:  -1.6793743e+17 Training accuracy:  0.5334\n",
      "Epoch  326 Batch size:  10000 Batch loss:  -1.7100658e+17 Training accuracy:  0.5334\n",
      "Epoch  327 Batch size:  10000 Batch loss:  -1.7411945e+17 Training accuracy:  0.5334\n",
      "Epoch  328 Batch size:  10000 Batch loss:  -1.7727854e+17 Training accuracy:  0.5334\n",
      "Epoch  329 Batch size:  10000 Batch loss:  -1.804838e+17 Training accuracy:  0.5334\n",
      "Epoch  330 Batch size:  10000 Batch loss:  -1.8373497e+17 Training accuracy:  0.5334\n",
      "Epoch  331 Batch size:  10000 Batch loss:  -1.8703267e+17 Training accuracy:  0.5334\n",
      "Epoch  332 Batch size:  10000 Batch loss:  -1.9037788e+17 Training accuracy:  0.5334\n",
      "Epoch  333 Batch size:  10000 Batch loss:  -1.9377097e+17 Training accuracy:  0.5334\n",
      "Epoch  334 Batch size:  10000 Batch loss:  -1.9721188e+17 Training accuracy:  0.5334\n",
      "Epoch  335 Batch size:  10000 Batch loss:  -2.0070193e+17 Training accuracy:  0.5334\n",
      "Epoch  336 Batch size:  10000 Batch loss:  -2.0424109e+17 Training accuracy:  0.5334\n",
      "Epoch  337 Batch size:  10000 Batch loss:  -2.0782995e+17 Training accuracy:  0.5334\n",
      "Epoch  338 Batch size:  10000 Batch loss:  -2.1146931e+17 Training accuracy:  0.5334\n",
      "Epoch  339 Batch size:  10000 Batch loss:  -2.1515989e+17 Training accuracy:  0.5334\n",
      "Epoch  340 Batch size:  10000 Batch loss:  -2.1890113e+17 Training accuracy:  0.5334\n",
      "Epoch  341 Batch size:  10000 Batch loss:  -2.2269474e+17 Training accuracy:  0.5334\n",
      "Epoch  342 Batch size:  10000 Batch loss:  -2.2654046e+17 Training accuracy:  0.5334\n",
      "Epoch  343 Batch size:  10000 Batch loss:  -2.3043907e+17 Training accuracy:  0.5334\n",
      "Epoch  344 Batch size:  10000 Batch loss:  -2.3439136e+17 Training accuracy:  0.5334\n",
      "Epoch  345 Batch size:  10000 Batch loss:  -2.3839752e+17 Training accuracy:  0.5334\n",
      "Epoch  346 Batch size:  10000 Batch loss:  -2.424589e+17 Training accuracy:  0.5334\n",
      "Epoch  347 Batch size:  10000 Batch loss:  -2.4657459e+17 Training accuracy:  0.5334\n",
      "Epoch  348 Batch size:  10000 Batch loss:  -2.5074619e+17 Training accuracy:  0.5334\n",
      "Epoch  349 Batch size:  10000 Batch loss:  -2.5497434e+17 Training accuracy:  0.5334\n",
      "Epoch  350 Batch size:  10000 Batch loss:  -2.5925905e+17 Training accuracy:  0.5334\n",
      "Epoch  351 Batch size:  10000 Batch loss:  -2.6360097e+17 Training accuracy:  0.5334\n",
      "Epoch  352 Batch size:  10000 Batch loss:  -2.6800072e+17 Training accuracy:  0.5334\n",
      "Epoch  353 Batch size:  10000 Batch loss:  -2.7245914e+17 Training accuracy:  0.5334\n",
      "Epoch  354 Batch size:  10000 Batch loss:  -2.7697653e+17 Training accuracy:  0.5334\n",
      "Epoch  355 Batch size:  10000 Batch loss:  -2.8155354e+17 Training accuracy:  0.5334\n",
      "Epoch  356 Batch size:  10000 Batch loss:  -2.8619066e+17 Training accuracy:  0.5334\n",
      "Epoch  357 Batch size:  10000 Batch loss:  -2.9088862e+17 Training accuracy:  0.5334\n",
      "Epoch  358 Batch size:  10000 Batch loss:  -2.956483e+17 Training accuracy:  0.5334\n",
      "Epoch  359 Batch size:  10000 Batch loss:  -3.0047024e+17 Training accuracy:  0.5334\n",
      "Epoch  360 Batch size:  10000 Batch loss:  -3.0535403e+17 Training accuracy:  0.5334\n",
      "Epoch  361 Batch size:  10000 Batch loss:  -3.103005e+17 Training accuracy:  0.5334\n",
      "Epoch  362 Batch size:  10000 Batch loss:  -3.1531234e+17 Training accuracy:  0.5334\n",
      "Epoch  363 Batch size:  10000 Batch loss:  -3.2038662e+17 Training accuracy:  0.5334\n",
      "Epoch  364 Batch size:  10000 Batch loss:  -3.2552773e+17 Training accuracy:  0.5334\n",
      "Epoch  365 Batch size:  10000 Batch loss:  -3.3073265e+17 Training accuracy:  0.5334\n",
      "Epoch  366 Batch size:  10000 Batch loss:  -3.3600598e+17 Training accuracy:  0.5334\n",
      "Epoch  367 Batch size:  10000 Batch loss:  -3.4134476e+17 Training accuracy:  0.5334\n",
      "Epoch  368 Batch size:  10000 Batch loss:  -3.4675013e+17 Training accuracy:  0.5334\n",
      "Epoch  369 Batch size:  10000 Batch loss:  -3.5222377e+17 Training accuracy:  0.5334\n",
      "Epoch  370 Batch size:  10000 Batch loss:  -3.5776638e+17 Training accuracy:  0.5334\n",
      "Epoch  371 Batch size:  10000 Batch loss:  -3.633795e+17 Training accuracy:  0.5334\n",
      "Epoch  372 Batch size:  10000 Batch loss:  -3.6906324e+17 Training accuracy:  0.5334\n",
      "Epoch  373 Batch size:  10000 Batch loss:  -3.7481465e+17 Training accuracy:  0.5334\n",
      "Epoch  374 Batch size:  10000 Batch loss:  -3.806371e+17 Training accuracy:  0.5334\n",
      "Epoch  375 Batch size:  10000 Batch loss:  -3.865348e+17 Training accuracy:  0.5334\n",
      "Epoch  376 Batch size:  10000 Batch loss:  -3.9250177e+17 Training accuracy:  0.5334\n",
      "Epoch  377 Batch size:  10000 Batch loss:  -3.985451e+17 Training accuracy:  0.5334\n",
      "Epoch  378 Batch size:  10000 Batch loss:  -4.0465976e+17 Training accuracy:  0.5334\n",
      "Epoch  379 Batch size:  10000 Batch loss:  -4.1084894e+17 Training accuracy:  0.5334\n",
      "Epoch  380 Batch size:  10000 Batch loss:  -4.1711375e+17 Training accuracy:  0.5334\n",
      "Epoch  381 Batch size:  10000 Batch loss:  -4.2345443e+17 Training accuracy:  0.5334\n",
      "Epoch  382 Batch size:  10000 Batch loss:  -4.2987263e+17 Training accuracy:  0.5334\n",
      "Epoch  383 Batch size:  10000 Batch loss:  -4.3636744e+17 Training accuracy:  0.5334\n",
      "Epoch  384 Batch size:  10000 Batch loss:  -4.4293922e+17 Training accuracy:  0.5334\n",
      "Epoch  385 Batch size:  10000 Batch loss:  -4.4959158e+17 Training accuracy:  0.5334\n",
      "Epoch  386 Batch size:  10000 Batch loss:  -4.5631942e+17 Training accuracy:  0.5334\n",
      "Epoch  387 Batch size:  10000 Batch loss:  -4.631326e+17 Training accuracy:  0.5334\n",
      "Epoch  388 Batch size:  10000 Batch loss:  -4.70021e+17 Training accuracy:  0.5334\n",
      "Epoch  389 Batch size:  10000 Batch loss:  -4.7699302e+17 Training accuracy:  0.5334\n",
      "Epoch  390 Batch size:  10000 Batch loss:  -4.8404728e+17 Training accuracy:  0.5334\n",
      "Epoch  391 Batch size:  10000 Batch loss:  -4.9118363e+17 Training accuracy:  0.5334\n",
      "Epoch  392 Batch size:  10000 Batch loss:  -4.9840336e+17 Training accuracy:  0.5334\n",
      "Epoch  393 Batch size:  10000 Batch loss:  -5.0570763e+17 Training accuracy:  0.5334\n",
      "Epoch  394 Batch size:  10000 Batch loss:  -5.1309703e+17 Training accuracy:  0.5334\n",
      "Epoch  395 Batch size:  10000 Batch loss:  -5.2057216e+17 Training accuracy:  0.5334\n",
      "Epoch  396 Batch size:  10000 Batch loss:  -5.28134e+17 Training accuracy:  0.5334\n",
      "Epoch  397 Batch size:  10000 Batch loss:  -5.357808e+17 Training accuracy:  0.5334\n",
      "Epoch  398 Batch size:  10000 Batch loss:  -5.4351722e+17 Training accuracy:  0.5334\n",
      "Epoch  399 Batch size:  10000 Batch loss:  -5.5134114e+17 Training accuracy:  0.5334\n",
      "Epoch  400 Batch size:  10000 Batch loss:  -5.5925446e+17 Training accuracy:  0.5334\n",
      "Epoch  401 Batch size:  10000 Batch loss:  -5.6725808e+17 Training accuracy:  0.5334\n",
      "Epoch  402 Batch size:  10000 Batch loss:  -5.7535286e+17 Training accuracy:  0.5334\n",
      "Epoch  403 Batch size:  10000 Batch loss:  -5.8353824e+17 Training accuracy:  0.5334\n",
      "Epoch  404 Batch size:  10000 Batch loss:  -5.918146e+17 Training accuracy:  0.5334\n",
      "Epoch  405 Batch size:  10000 Batch loss:  -6.001858e+17 Training accuracy:  0.5334\n",
      "Epoch  406 Batch size:  10000 Batch loss:  -6.0865026e+17 Training accuracy:  0.5334\n",
      "Epoch  407 Batch size:  10000 Batch loss:  -6.172094e+17 Training accuracy:  0.5334\n",
      "Epoch  408 Batch size:  10000 Batch loss:  -6.258632e+17 Training accuracy:  0.5334\n",
      "Epoch  409 Batch size:  10000 Batch loss:  -6.3461324e+17 Training accuracy:  0.5334\n",
      "Epoch  410 Batch size:  10000 Batch loss:  -6.434599e+17 Training accuracy:  0.5334\n",
      "Epoch  411 Batch size:  10000 Batch loss:  -6.524054e+17 Training accuracy:  0.5334\n",
      "Epoch  412 Batch size:  10000 Batch loss:  -6.6144716e+17 Training accuracy:  0.5334\n",
      "Epoch  413 Batch size:  10000 Batch loss:  -6.7058994e+17 Training accuracy:  0.5334\n",
      "Epoch  414 Batch size:  10000 Batch loss:  -6.798332e+17 Training accuracy:  0.5334\n",
      "Epoch  415 Batch size:  10000 Batch loss:  -6.891738e+17 Training accuracy:  0.5334\n",
      "Epoch  416 Batch size:  10000 Batch loss:  -6.986173e+17 Training accuracy:  0.5334\n",
      "Epoch  417 Batch size:  10000 Batch loss:  -7.081664e+17 Training accuracy:  0.5334\n",
      "Epoch  418 Batch size:  10000 Batch loss:  -7.178171e+17 Training accuracy:  0.5334\n",
      "Epoch  419 Batch size:  10000 Batch loss:  -7.2757035e+17 Training accuracy:  0.5334\n",
      "Epoch  420 Batch size:  10000 Batch loss:  -7.3742816e+17 Training accuracy:  0.5334\n",
      "Epoch  421 Batch size:  10000 Batch loss:  -7.473924e+17 Training accuracy:  0.5334\n",
      "Epoch  422 Batch size:  10000 Batch loss:  -7.5746346e+17 Training accuracy:  0.5334\n",
      "Epoch  423 Batch size:  10000 Batch loss:  -7.676416e+17 Training accuracy:  0.5334\n",
      "Epoch  424 Batch size:  10000 Batch loss:  -7.779265e+17 Training accuracy:  0.5334\n",
      "Epoch  425 Batch size:  10000 Batch loss:  -7.8831974e+17 Training accuracy:  0.5334\n",
      "Epoch  426 Batch size:  10000 Batch loss:  -7.988246e+17 Training accuracy:  0.5334\n",
      "Epoch  427 Batch size:  10000 Batch loss:  -8.0943916e+17 Training accuracy:  0.5334\n",
      "Epoch  428 Batch size:  10000 Batch loss:  -8.201663e+17 Training accuracy:  0.5334\n",
      "Epoch  429 Batch size:  10000 Batch loss:  -8.310038e+17 Training accuracy:  0.5334\n",
      "Epoch  430 Batch size:  10000 Batch loss:  -8.419576e+17 Training accuracy:  0.5334\n",
      "Epoch  431 Batch size:  10000 Batch loss:  -8.5302e+17 Training accuracy:  0.5334\n",
      "Epoch  432 Batch size:  10000 Batch loss:  -8.6420404e+17 Training accuracy:  0.5334\n",
      "Epoch  433 Batch size:  10000 Batch loss:  -8.7549926e+17 Training accuracy:  0.5334\n",
      "Epoch  434 Batch size:  10000 Batch loss:  -8.8691054e+17 Training accuracy:  0.5334\n",
      "Epoch  435 Batch size:  10000 Batch loss:  -8.984416e+17 Training accuracy:  0.5334\n",
      "Epoch  436 Batch size:  10000 Batch loss:  -9.100912e+17 Training accuracy:  0.5334\n",
      "Epoch  437 Batch size:  10000 Batch loss:  -9.218596e+17 Training accuracy:  0.5334\n",
      "Epoch  438 Batch size:  10000 Batch loss:  -9.337483e+17 Training accuracy:  0.5334\n",
      "Epoch  439 Batch size:  10000 Batch loss:  -9.4575736e+17 Training accuracy:  0.5334\n",
      "Epoch  440 Batch size:  10000 Batch loss:  -9.5788615e+17 Training accuracy:  0.5334\n",
      "Epoch  441 Batch size:  10000 Batch loss:  -9.7013684e+17 Training accuracy:  0.5334\n",
      "Epoch  442 Batch size:  10000 Batch loss:  -9.825145e+17 Training accuracy:  0.5334\n",
      "Epoch  443 Batch size:  10000 Batch loss:  -9.950192e+17 Training accuracy:  0.5334\n",
      "Epoch  444 Batch size:  10000 Batch loss:  -1.0076428e+18 Training accuracy:  0.5334\n",
      "Epoch  445 Batch size:  10000 Batch loss:  -1.0203976e+18 Training accuracy:  0.5334\n",
      "Epoch  446 Batch size:  10000 Batch loss:  -1.0332782e+18 Training accuracy:  0.5334\n",
      "Epoch  447 Batch size:  10000 Batch loss:  -1.04628654e+18 Training accuracy:  0.5334\n",
      "Epoch  448 Batch size:  10000 Batch loss:  -1.05942495e+18 Training accuracy:  0.5334\n",
      "Epoch  449 Batch size:  10000 Batch loss:  -1.07269674e+18 Training accuracy:  0.5334\n",
      "Epoch  450 Batch size:  10000 Batch loss:  -1.086089e+18 Training accuracy:  0.5334\n",
      "Epoch  451 Batch size:  10000 Batch loss:  -1.0996167e+18 Training accuracy:  0.5334\n",
      "Epoch  452 Batch size:  10000 Batch loss:  -1.11328205e+18 Training accuracy:  0.5334\n",
      "Epoch  453 Batch size:  10000 Batch loss:  -1.12708106e+18 Training accuracy:  0.5334\n",
      "Epoch  454 Batch size:  10000 Batch loss:  -1.1410135e+18 Training accuracy:  0.5334\n",
      "Epoch  455 Batch size:  10000 Batch loss:  -1.15508e+18 Training accuracy:  0.5334\n",
      "Epoch  456 Batch size:  10000 Batch loss:  -1.1692833e+18 Training accuracy:  0.5334\n",
      "Epoch  457 Batch size:  10000 Batch loss:  -1.1836287e+18 Training accuracy:  0.5334\n",
      "Epoch  458 Batch size:  10000 Batch loss:  -1.1981165e+18 Training accuracy:  0.5334\n",
      "Epoch  459 Batch size:  10000 Batch loss:  -1.2127329e+18 Training accuracy:  0.5334\n",
      "Epoch  460 Batch size:  10000 Batch loss:  -1.2274915e+18 Training accuracy:  0.5334\n",
      "Epoch  461 Batch size:  10000 Batch loss:  -1.242395e+18 Training accuracy:  0.5334\n",
      "Epoch  462 Batch size:  10000 Batch loss:  -1.2574418e+18 Training accuracy:  0.5334\n",
      "Epoch  463 Batch size:  10000 Batch loss:  -1.2726286e+18 Training accuracy:  0.5334\n",
      "Epoch  464 Batch size:  10000 Batch loss:  -1.2879576e+18 Training accuracy:  0.5334\n",
      "Epoch  465 Batch size:  10000 Batch loss:  -1.3034393e+18 Training accuracy:  0.5334\n",
      "Epoch  466 Batch size:  10000 Batch loss:  -1.3190654e+18 Training accuracy:  0.5334\n",
      "Epoch  467 Batch size:  10000 Batch loss:  -1.3348375e+18 Training accuracy:  0.5334\n",
      "Epoch  468 Batch size:  10000 Batch loss:  -1.3507584e+18 Training accuracy:  0.5334\n",
      "Epoch  469 Batch size:  10000 Batch loss:  -1.3668297e+18 Training accuracy:  0.5334\n",
      "Epoch  470 Batch size:  10000 Batch loss:  -1.3830496e+18 Training accuracy:  0.5334\n",
      "Epoch  471 Batch size:  10000 Batch loss:  -1.3994159e+18 Training accuracy:  0.5334\n",
      "Epoch  472 Batch size:  10000 Batch loss:  -1.4159376e+18 Training accuracy:  0.5334\n",
      "Epoch  473 Batch size:  10000 Batch loss:  -1.4326177e+18 Training accuracy:  0.5334\n",
      "Epoch  474 Batch size:  10000 Batch loss:  -1.4494495e+18 Training accuracy:  0.5334\n",
      "Epoch  475 Batch size:  10000 Batch loss:  -1.4664376e+18 Training accuracy:  0.5334\n",
      "Epoch  476 Batch size:  10000 Batch loss:  -1.4835787e+18 Training accuracy:  0.5334\n",
      "Epoch  477 Batch size:  10000 Batch loss:  -1.5008846e+18 Training accuracy:  0.5334\n",
      "Epoch  478 Batch size:  10000 Batch loss:  -1.5183423e+18 Training accuracy:  0.5334\n",
      "Epoch  479 Batch size:  10000 Batch loss:  -1.5359599e+18 Training accuracy:  0.5334\n",
      "Epoch  480 Batch size:  10000 Batch loss:  -1.5537443e+18 Training accuracy:  0.5334\n",
      "Epoch  481 Batch size:  10000 Batch loss:  -1.5716832e+18 Training accuracy:  0.5334\n",
      "Epoch  482 Batch size:  10000 Batch loss:  -1.5897968e+18 Training accuracy:  0.5334\n",
      "Epoch  483 Batch size:  10000 Batch loss:  -1.6080546e+18 Training accuracy:  0.5334\n",
      "Epoch  484 Batch size:  10000 Batch loss:  -1.6264994e+18 Training accuracy:  0.5334\n",
      "Epoch  485 Batch size:  10000 Batch loss:  -1.645092e+18 Training accuracy:  0.5334\n",
      "Epoch  486 Batch size:  10000 Batch loss:  -1.6638596e+18 Training accuracy:  0.5334\n",
      "Epoch  487 Batch size:  10000 Batch loss:  -1.6828039e+18 Training accuracy:  0.5334\n",
      "Epoch  488 Batch size:  10000 Batch loss:  -1.7019001e+18 Training accuracy:  0.5334\n",
      "Epoch  489 Batch size:  10000 Batch loss:  -1.7211734e+18 Training accuracy:  0.5334\n",
      "Epoch  490 Batch size:  10000 Batch loss:  -1.7406264e+18 Training accuracy:  0.5334\n",
      "Epoch  491 Batch size:  10000 Batch loss:  -1.7602504e+18 Training accuracy:  0.5334\n",
      "Epoch  492 Batch size:  10000 Batch loss:  -1.7800413e+18 Training accuracy:  0.5334\n",
      "Epoch  493 Batch size:  10000 Batch loss:  -1.8000053e+18 Training accuracy:  0.5334\n",
      "Epoch  494 Batch size:  10000 Batch loss:  -1.820147e+18 Training accuracy:  0.5334\n",
      "Epoch  495 Batch size:  10000 Batch loss:  -1.8404674e+18 Training accuracy:  0.5334\n",
      "Epoch  496 Batch size:  10000 Batch loss:  -1.8609684e+18 Training accuracy:  0.5334\n",
      "Epoch  497 Batch size:  10000 Batch loss:  -1.8816494e+18 Training accuracy:  0.5334\n",
      "Epoch  498 Batch size:  10000 Batch loss:  -1.9025074e+18 Training accuracy:  0.5334\n",
      "Epoch  499 Batch size:  10000 Batch loss:  -1.9235435e+18 Training accuracy:  0.5334\n",
      "Epoch  500 Batch size:  10000 Batch loss:  -1.9447604e+18 Training accuracy:  0.5334\n",
      "Epoch  501 Batch size:  10000 Batch loss:  -1.9661677e+18 Training accuracy:  0.5334\n",
      "Epoch  502 Batch size:  10000 Batch loss:  -1.9877668e+18 Training accuracy:  0.5334\n",
      "Epoch  503 Batch size:  10000 Batch loss:  -2.0095458e+18 Training accuracy:  0.5334\n",
      "Epoch  504 Batch size:  10000 Batch loss:  -2.0314981e+18 Training accuracy:  0.5334\n",
      "Epoch  505 Batch size:  10000 Batch loss:  -2.0536586e+18 Training accuracy:  0.5334\n",
      "Epoch  506 Batch size:  10000 Batch loss:  -2.076e+18 Training accuracy:  0.5334\n",
      "Epoch  507 Batch size:  10000 Batch loss:  -2.0985315e+18 Training accuracy:  0.5334\n",
      "Epoch  508 Batch size:  10000 Batch loss:  -2.1212536e+18 Training accuracy:  0.5334\n",
      "Epoch  509 Batch size:  10000 Batch loss:  -2.1441802e+18 Training accuracy:  0.5334\n",
      "Epoch  510 Batch size:  10000 Batch loss:  -2.1672754e+18 Training accuracy:  0.5334\n",
      "Epoch  511 Batch size:  10000 Batch loss:  -2.190598e+18 Training accuracy:  0.5334\n",
      "Epoch  512 Batch size:  10000 Batch loss:  -2.2140993e+18 Training accuracy:  0.5334\n",
      "Epoch  513 Batch size:  10000 Batch loss:  -2.2377878e+18 Training accuracy:  0.5334\n",
      "Epoch  514 Batch size:  10000 Batch loss:  -2.2616979e+18 Training accuracy:  0.5334\n",
      "Epoch  515 Batch size:  10000 Batch loss:  -2.285802e+18 Training accuracy:  0.5334\n",
      "Epoch  516 Batch size:  10000 Batch loss:  -2.3100995e+18 Training accuracy:  0.5334\n",
      "Epoch  517 Batch size:  10000 Batch loss:  -2.3346002e+18 Training accuracy:  0.5334\n",
      "Epoch  518 Batch size:  10000 Batch loss:  -2.3593065e+18 Training accuracy:  0.5334\n",
      "Epoch  519 Batch size:  10000 Batch loss:  -2.3842184e+18 Training accuracy:  0.5334\n",
      "Epoch  520 Batch size:  10000 Batch loss:  -2.409337e+18 Training accuracy:  0.5334\n",
      "Epoch  521 Batch size:  10000 Batch loss:  -2.4346632e+18 Training accuracy:  0.5334\n",
      "Epoch  522 Batch size:  10000 Batch loss:  -2.4601988e+18 Training accuracy:  0.5334\n",
      "Epoch  523 Batch size:  10000 Batch loss:  -2.4859447e+18 Training accuracy:  0.5334\n",
      "Epoch  524 Batch size:  10000 Batch loss:  -2.5119006e+18 Training accuracy:  0.5334\n",
      "Epoch  525 Batch size:  10000 Batch loss:  -2.5380667e+18 Training accuracy:  0.5334\n",
      "Epoch  526 Batch size:  10000 Batch loss:  -2.56445e+18 Training accuracy:  0.5334\n",
      "Epoch  527 Batch size:  10000 Batch loss:  -2.5910478e+18 Training accuracy:  0.5334\n",
      "Epoch  528 Batch size:  10000 Batch loss:  -2.6178492e+18 Training accuracy:  0.5334\n",
      "Epoch  529 Batch size:  10000 Batch loss:  -2.6448881e+18 Training accuracy:  0.5334\n",
      "Epoch  530 Batch size:  10000 Batch loss:  -2.6721324e+18 Training accuracy:  0.5334\n",
      "Epoch  531 Batch size:  10000 Batch loss:  -2.6995965e+18 Training accuracy:  0.5334\n",
      "Epoch  532 Batch size:  10000 Batch loss:  -2.7272888e+18 Training accuracy:  0.5334\n",
      "Epoch  533 Batch size:  10000 Batch loss:  -2.7551972e+18 Training accuracy:  0.5334\n",
      "Epoch  534 Batch size:  10000 Batch loss:  -2.7833293e+18 Training accuracy:  0.5334\n",
      "Epoch  535 Batch size:  10000 Batch loss:  -2.811696e+18 Training accuracy:  0.5334\n",
      "Epoch  536 Batch size:  10000 Batch loss:  -2.8402708e+18 Training accuracy:  0.5334\n",
      "Epoch  537 Batch size:  10000 Batch loss:  -2.8690852e+18 Training accuracy:  0.5334\n",
      "Epoch  538 Batch size:  10000 Batch loss:  -2.8981304e+18 Training accuracy:  0.5334\n",
      "Epoch  539 Batch size:  10000 Batch loss:  -2.9274013e+18 Training accuracy:  0.5334\n",
      "Epoch  540 Batch size:  10000 Batch loss:  -2.9569012e+18 Training accuracy:  0.5334\n",
      "Epoch  541 Batch size:  10000 Batch loss:  -2.9866337e+18 Training accuracy:  0.5334\n",
      "Epoch  542 Batch size:  10000 Batch loss:  -3.0166006e+18 Training accuracy:  0.5334\n",
      "Epoch  543 Batch size:  10000 Batch loss:  -3.0468044e+18 Training accuracy:  0.5334\n",
      "Epoch  544 Batch size:  10000 Batch loss:  -3.077247e+18 Training accuracy:  0.5334\n",
      "Epoch  545 Batch size:  10000 Batch loss:  -3.107929e+18 Training accuracy:  0.5334\n",
      "Epoch  546 Batch size:  10000 Batch loss:  -3.138848e+18 Training accuracy:  0.5334\n",
      "Epoch  547 Batch size:  10000 Batch loss:  -3.1700017e+18 Training accuracy:  0.5334\n",
      "Epoch  548 Batch size:  10000 Batch loss:  -3.201398e+18 Training accuracy:  0.5334\n",
      "Epoch  549 Batch size:  10000 Batch loss:  -3.233047e+18 Training accuracy:  0.5334\n",
      "Epoch  550 Batch size:  10000 Batch loss:  -3.2649335e+18 Training accuracy:  0.5334\n",
      "Epoch  551 Batch size:  10000 Batch loss:  -3.2970602e+18 Training accuracy:  0.5334\n",
      "Epoch  552 Batch size:  10000 Batch loss:  -3.3294504e+18 Training accuracy:  0.5334\n",
      "Epoch  553 Batch size:  10000 Batch loss:  -3.36207e+18 Training accuracy:  0.5334\n",
      "Epoch  554 Batch size:  10000 Batch loss:  -3.3949538e+18 Training accuracy:  0.5334\n",
      "Epoch  555 Batch size:  10000 Batch loss:  -3.4280807e+18 Training accuracy:  0.5334\n",
      "Epoch  556 Batch size:  10000 Batch loss:  -3.4614704e+18 Training accuracy:  0.5334\n",
      "Epoch  557 Batch size:  10000 Batch loss:  -3.495094e+18 Training accuracy:  0.5334\n",
      "Epoch  558 Batch size:  10000 Batch loss:  -3.5289892e+18 Training accuracy:  0.5334\n",
      "Epoch  559 Batch size:  10000 Batch loss:  -3.563149e+18 Training accuracy:  0.5334\n",
      "Epoch  560 Batch size:  10000 Batch loss:  -3.5975548e+18 Training accuracy:  0.5334\n",
      "Epoch  561 Batch size:  10000 Batch loss:  -3.632214e+18 Training accuracy:  0.5334\n",
      "Epoch  562 Batch size:  10000 Batch loss:  -3.6671374e+18 Training accuracy:  0.5334\n",
      "Epoch  563 Batch size:  10000 Batch loss:  -3.702324e+18 Training accuracy:  0.5334\n",
      "Epoch  564 Batch size:  10000 Batch loss:  -3.7377788e+18 Training accuracy:  0.5334\n",
      "Epoch  565 Batch size:  10000 Batch loss:  -3.7735016e+18 Training accuracy:  0.5334\n",
      "Epoch  566 Batch size:  10000 Batch loss:  -3.809485e+18 Training accuracy:  0.5334\n",
      "Epoch  567 Batch size:  10000 Batch loss:  -3.8457547e+18 Training accuracy:  0.5334\n",
      "Epoch  568 Batch size:  10000 Batch loss:  -3.8822587e+18 Training accuracy:  0.5334\n",
      "Epoch  569 Batch size:  10000 Batch loss:  -3.9190404e+18 Training accuracy:  0.5334\n",
      "Epoch  570 Batch size:  10000 Batch loss:  -3.9561278e+18 Training accuracy:  0.5334\n",
      "Epoch  571 Batch size:  10000 Batch loss:  -3.9934562e+18 Training accuracy:  0.5334\n",
      "Epoch  572 Batch size:  10000 Batch loss:  -4.0310587e+18 Training accuracy:  0.5334\n",
      "Epoch  573 Batch size:  10000 Batch loss:  -4.0689564e+18 Training accuracy:  0.5334\n",
      "Epoch  574 Batch size:  10000 Batch loss:  -4.1071342e+18 Training accuracy:  0.5334\n",
      "Epoch  575 Batch size:  10000 Batch loss:  -4.1455756e+18 Training accuracy:  0.5334\n",
      "Epoch  576 Batch size:  10000 Batch loss:  -4.1842962e+18 Training accuracy:  0.5334\n",
      "Epoch  577 Batch size:  10000 Batch loss:  -4.223306e+18 Training accuracy:  0.5334\n",
      "Epoch  578 Batch size:  10000 Batch loss:  -4.262603e+18 Training accuracy:  0.5334\n",
      "Epoch  579 Batch size:  10000 Batch loss:  -4.3021892e+18 Training accuracy:  0.5334\n",
      "Epoch  580 Batch size:  10000 Batch loss:  -4.3420619e+18 Training accuracy:  0.5334\n",
      "Epoch  581 Batch size:  10000 Batch loss:  -4.3822196e+18 Training accuracy:  0.5334\n",
      "Epoch  582 Batch size:  10000 Batch loss:  -4.4226662e+18 Training accuracy:  0.5334\n",
      "Epoch  583 Batch size:  10000 Batch loss:  -4.4634042e+18 Training accuracy:  0.5334\n",
      "Epoch  584 Batch size:  10000 Batch loss:  -4.5044328e+18 Training accuracy:  0.5334\n",
      "Epoch  585 Batch size:  10000 Batch loss:  -4.5457516e+18 Training accuracy:  0.5334\n",
      "Epoch  586 Batch size:  10000 Batch loss:  -4.5873577e+18 Training accuracy:  0.5334\n",
      "Epoch  587 Batch size:  10000 Batch loss:  -4.6292535e+18 Training accuracy:  0.5334\n",
      "Epoch  588 Batch size:  10000 Batch loss:  -4.671451e+18 Training accuracy:  0.5334\n",
      "Epoch  589 Batch size:  10000 Batch loss:  -4.713962e+18 Training accuracy:  0.5334\n",
      "Epoch  590 Batch size:  10000 Batch loss:  -4.7567886e+18 Training accuracy:  0.5334\n",
      "Epoch  591 Batch size:  10000 Batch loss:  -4.799914e+18 Training accuracy:  0.5334\n",
      "Epoch  592 Batch size:  10000 Batch loss:  -4.843336e+18 Training accuracy:  0.5334\n",
      "Epoch  593 Batch size:  10000 Batch loss:  -4.8870587e+18 Training accuracy:  0.5334\n",
      "Epoch  594 Batch size:  10000 Batch loss:  -4.931084e+18 Training accuracy:  0.5334\n",
      "Epoch  595 Batch size:  10000 Batch loss:  -4.975427e+18 Training accuracy:  0.5334\n",
      "Epoch  596 Batch size:  10000 Batch loss:  -5.0201024e+18 Training accuracy:  0.5334\n",
      "Epoch  597 Batch size:  10000 Batch loss:  -5.065044e+18 Training accuracy:  0.5334\n",
      "Epoch  598 Batch size:  10000 Batch loss:  -5.110337e+18 Training accuracy:  0.5334\n",
      "Epoch  599 Batch size:  10000 Batch loss:  -5.1559454e+18 Training accuracy:  0.5334\n",
      "Epoch  600 Batch size:  10000 Batch loss:  -5.201824e+18 Training accuracy:  0.5334\n",
      "Epoch  601 Batch size:  10000 Batch loss:  -5.2480943e+18 Training accuracy:  0.5334\n",
      "Epoch  602 Batch size:  10000 Batch loss:  -5.2946086e+18 Training accuracy:  0.5334\n",
      "Epoch  603 Batch size:  10000 Batch loss:  -5.3415045e+18 Training accuracy:  0.5334\n",
      "Epoch  604 Batch size:  10000 Batch loss:  -5.388698e+18 Training accuracy:  0.5334\n",
      "Epoch  605 Batch size:  10000 Batch loss:  -5.436189e+18 Training accuracy:  0.5334\n",
      "Epoch  606 Batch size:  10000 Batch loss:  -5.4840803e+18 Training accuracy:  0.5334\n",
      "Epoch  607 Batch size:  10000 Batch loss:  -5.5322274e+18 Training accuracy:  0.5334\n",
      "Epoch  608 Batch size:  10000 Batch loss:  -5.5807186e+18 Training accuracy:  0.5334\n",
      "Epoch  609 Batch size:  10000 Batch loss:  -5.629587e+18 Training accuracy:  0.5334\n",
      "Epoch  610 Batch size:  10000 Batch loss:  -5.67875e+18 Training accuracy:  0.5334\n",
      "Epoch  611 Batch size:  10000 Batch loss:  -5.72823e+18 Training accuracy:  0.5334\n",
      "Epoch  612 Batch size:  10000 Batch loss:  -5.778054e+18 Training accuracy:  0.5334\n",
      "Epoch  613 Batch size:  10000 Batch loss:  -5.8282407e+18 Training accuracy:  0.5334\n",
      "Epoch  614 Batch size:  10000 Batch loss:  -5.8787594e+18 Training accuracy:  0.5334\n",
      "Epoch  615 Batch size:  10000 Batch loss:  -5.9296107e+18 Training accuracy:  0.5334\n",
      "Epoch  616 Batch size:  10000 Batch loss:  -5.980803e+18 Training accuracy:  0.5334\n",
      "Epoch  617 Batch size:  10000 Batch loss:  -6.032338e+18 Training accuracy:  0.5334\n",
      "Epoch  618 Batch size:  10000 Batch loss:  -6.0842174e+18 Training accuracy:  0.5334\n",
      "Epoch  619 Batch size:  10000 Batch loss:  -6.136445e+18 Training accuracy:  0.5334\n",
      "Epoch  620 Batch size:  10000 Batch loss:  -6.1890256e+18 Training accuracy:  0.5334\n",
      "Epoch  621 Batch size:  10000 Batch loss:  -6.2419665e+18 Training accuracy:  0.5334\n",
      "Epoch  622 Batch size:  10000 Batch loss:  -6.295267e+18 Training accuracy:  0.5334\n",
      "Epoch  623 Batch size:  10000 Batch loss:  -6.348917e+18 Training accuracy:  0.5334\n",
      "Epoch  624 Batch size:  10000 Batch loss:  -6.402909e+18 Training accuracy:  0.5334\n",
      "Epoch  625 Batch size:  10000 Batch loss:  -6.4572515e+18 Training accuracy:  0.5334\n",
      "Epoch  626 Batch size:  10000 Batch loss:  -6.511955e+18 Training accuracy:  0.5334\n",
      "Epoch  627 Batch size:  10000 Batch loss:  -6.5670157e+18 Training accuracy:  0.5334\n",
      "Epoch  628 Batch size:  10000 Batch loss:  -6.6224575e+18 Training accuracy:  0.5334\n",
      "Epoch  629 Batch size:  10000 Batch loss:  -6.6782517e+18 Training accuracy:  0.5334\n",
      "Epoch  630 Batch size:  10000 Batch loss:  -6.7344054e+18 Training accuracy:  0.5334\n",
      "Epoch  631 Batch size:  10000 Batch loss:  -6.790945e+18 Training accuracy:  0.5334\n",
      "Epoch  632 Batch size:  10000 Batch loss:  -6.847815e+18 Training accuracy:  0.5334\n",
      "Epoch  633 Batch size:  10000 Batch loss:  -6.9051205e+18 Training accuracy:  0.5334\n",
      "Epoch  634 Batch size:  10000 Batch loss:  -6.9627085e+18 Training accuracy:  0.5334\n",
      "Epoch  635 Batch size:  10000 Batch loss:  -7.020777e+18 Training accuracy:  0.5334\n",
      "Epoch  636 Batch size:  10000 Batch loss:  -7.079104e+18 Training accuracy:  0.5334\n",
      "Epoch  637 Batch size:  10000 Batch loss:  -7.1379267e+18 Training accuracy:  0.5334\n",
      "Epoch  638 Batch size:  10000 Batch loss:  -7.1970254e+18 Training accuracy:  0.5334\n",
      "Epoch  639 Batch size:  10000 Batch loss:  -7.2565645e+18 Training accuracy:  0.5334\n",
      "Epoch  640 Batch size:  10000 Batch loss:  -7.3164995e+18 Training accuracy:  0.5334\n",
      "Epoch  641 Batch size:  10000 Batch loss:  -7.376735e+18 Training accuracy:  0.5334\n",
      "Epoch  642 Batch size:  10000 Batch loss:  -7.4374386e+18 Training accuracy:  0.5334\n",
      "Epoch  643 Batch size:  10000 Batch loss:  -7.498532e+18 Training accuracy:  0.5334\n",
      "Epoch  644 Batch size:  10000 Batch loss:  -7.559963e+18 Training accuracy:  0.5334\n",
      "Epoch  645 Batch size:  10000 Batch loss:  -7.6217805e+18 Training accuracy:  0.5334\n",
      "Epoch  646 Batch size:  10000 Batch loss:  -7.684017e+18 Training accuracy:  0.5334\n",
      "Epoch  647 Batch size:  10000 Batch loss:  -7.7466614e+18 Training accuracy:  0.5334\n",
      "Epoch  648 Batch size:  10000 Batch loss:  -7.809699e+18 Training accuracy:  0.5334\n",
      "Epoch  649 Batch size:  10000 Batch loss:  -7.873129e+18 Training accuracy:  0.5334\n",
      "Epoch  650 Batch size:  10000 Batch loss:  -7.936951e+18 Training accuracy:  0.5334\n",
      "Epoch  651 Batch size:  10000 Batch loss:  -8.001175e+18 Training accuracy:  0.5334\n",
      "Epoch  652 Batch size:  10000 Batch loss:  -8.065802e+18 Training accuracy:  0.5334\n",
      "Epoch  653 Batch size:  10000 Batch loss:  -8.1308374e+18 Training accuracy:  0.5334\n",
      "Epoch  654 Batch size:  10000 Batch loss:  -8.1962836e+18 Training accuracy:  0.5334\n",
      "Epoch  655 Batch size:  10000 Batch loss:  -8.26214e+18 Training accuracy:  0.5334\n",
      "Epoch  656 Batch size:  10000 Batch loss:  -8.328396e+18 Training accuracy:  0.5334\n",
      "Epoch  657 Batch size:  10000 Batch loss:  -8.3950736e+18 Training accuracy:  0.5334\n",
      "Epoch  658 Batch size:  10000 Batch loss:  -8.46217e+18 Training accuracy:  0.5334\n",
      "Epoch  659 Batch size:  10000 Batch loss:  -8.5296594e+18 Training accuracy:  0.5334\n",
      "Epoch  660 Batch size:  10000 Batch loss:  -8.597588e+18 Training accuracy:  0.5334\n",
      "Epoch  661 Batch size:  10000 Batch loss:  -8.665937e+18 Training accuracy:  0.5334\n",
      "Epoch  662 Batch size:  10000 Batch loss:  -8.734681e+18 Training accuracy:  0.5334\n",
      "Epoch  663 Batch size:  10000 Batch loss:  -8.8038825e+18 Training accuracy:  0.5334\n",
      "Epoch  664 Batch size:  10000 Batch loss:  -8.873492e+18 Training accuracy:  0.5334\n",
      "Epoch  665 Batch size:  10000 Batch loss:  -8.943515e+18 Training accuracy:  0.5334\n",
      "Epoch  666 Batch size:  10000 Batch loss:  -9.0139926e+18 Training accuracy:  0.5334\n",
      "Epoch  667 Batch size:  10000 Batch loss:  -9.084896e+18 Training accuracy:  0.5334\n",
      "Epoch  668 Batch size:  10000 Batch loss:  -9.156217e+18 Training accuracy:  0.5334\n",
      "Epoch  669 Batch size:  10000 Batch loss:  -9.227971e+18 Training accuracy:  0.5334\n",
      "Epoch  670 Batch size:  10000 Batch loss:  -9.300173e+18 Training accuracy:  0.5334\n",
      "Epoch  671 Batch size:  10000 Batch loss:  -9.372821e+18 Training accuracy:  0.5334\n",
      "Epoch  672 Batch size:  10000 Batch loss:  -9.445904e+18 Training accuracy:  0.5334\n",
      "Epoch  673 Batch size:  10000 Batch loss:  -9.519421e+18 Training accuracy:  0.5334\n",
      "Epoch  674 Batch size:  10000 Batch loss:  -9.59337e+18 Training accuracy:  0.5334\n",
      "Epoch  675 Batch size:  10000 Batch loss:  -9.667763e+18 Training accuracy:  0.5334\n",
      "Epoch  676 Batch size:  10000 Batch loss:  -9.74261e+18 Training accuracy:  0.5334\n",
      "Epoch  677 Batch size:  10000 Batch loss:  -9.817913e+18 Training accuracy:  0.5334\n",
      "Epoch  678 Batch size:  10000 Batch loss:  -9.893666e+18 Training accuracy:  0.5334\n",
      "Epoch  679 Batch size:  10000 Batch loss:  -9.969874e+18 Training accuracy:  0.5334\n",
      "Epoch  680 Batch size:  10000 Batch loss:  -1.0046537e+19 Training accuracy:  0.5334\n",
      "Epoch  681 Batch size:  10000 Batch loss:  -1.0123657e+19 Training accuracy:  0.5334\n",
      "Epoch  682 Batch size:  10000 Batch loss:  -1.0201235e+19 Training accuracy:  0.5334\n",
      "Epoch  683 Batch size:  10000 Batch loss:  -1.0279279e+19 Training accuracy:  0.5334\n",
      "Epoch  684 Batch size:  10000 Batch loss:  -1.0357794e+19 Training accuracy:  0.5334\n",
      "Epoch  685 Batch size:  10000 Batch loss:  -1.0436775e+19 Training accuracy:  0.5334\n",
      "Epoch  686 Batch size:  10000 Batch loss:  -1.0516206e+19 Training accuracy:  0.5334\n",
      "Epoch  687 Batch size:  10000 Batch loss:  -1.0596082e+19 Training accuracy:  0.5334\n",
      "Epoch  688 Batch size:  10000 Batch loss:  -1.0676411e+19 Training accuracy:  0.5334\n",
      "Epoch  689 Batch size:  10000 Batch loss:  -1.0757242e+19 Training accuracy:  0.5334\n",
      "Epoch  690 Batch size:  10000 Batch loss:  -1.0838596e+19 Training accuracy:  0.5334\n",
      "Epoch  691 Batch size:  10000 Batch loss:  -1.0920436e+19 Training accuracy:  0.5334\n",
      "Epoch  692 Batch size:  10000 Batch loss:  -1.1002678e+19 Training accuracy:  0.5334\n",
      "Epoch  693 Batch size:  10000 Batch loss:  -1.1085355e+19 Training accuracy:  0.5334\n",
      "Epoch  694 Batch size:  10000 Batch loss:  -1.11686e+19 Training accuracy:  0.5334\n",
      "Epoch  695 Batch size:  10000 Batch loss:  -1.1252382e+19 Training accuracy:  0.5334\n",
      "Epoch  696 Batch size:  10000 Batch loss:  -1.1336554e+19 Training accuracy:  0.5334\n",
      "Epoch  697 Batch size:  10000 Batch loss:  -1.1421172e+19 Training accuracy:  0.5334\n",
      "Epoch  698 Batch size:  10000 Batch loss:  -1.1506413e+19 Training accuracy:  0.5334\n",
      "Epoch  699 Batch size:  10000 Batch loss:  -1.1592092e+19 Training accuracy:  0.5334\n",
      "Epoch  700 Batch size:  10000 Batch loss:  -1.1678169e+19 Training accuracy:  0.5334\n",
      "Epoch  701 Batch size:  10000 Batch loss:  -1.1764901e+19 Training accuracy:  0.5334\n",
      "Epoch  702 Batch size:  10000 Batch loss:  -1.1852047e+19 Training accuracy:  0.5334\n",
      "Epoch  703 Batch size:  10000 Batch loss:  -1.1939648e+19 Training accuracy:  0.5334\n",
      "Epoch  704 Batch size:  10000 Batch loss:  -1.2027888e+19 Training accuracy:  0.5334\n",
      "Epoch  705 Batch size:  10000 Batch loss:  -1.2116419e+19 Training accuracy:  0.5334\n",
      "Epoch  706 Batch size:  10000 Batch loss:  -1.2205706e+19 Training accuracy:  0.5334\n",
      "Epoch  707 Batch size:  10000 Batch loss:  -1.2295245e+19 Training accuracy:  0.5334\n",
      "Epoch  708 Batch size:  10000 Batch loss:  -1.2385548e+19 Training accuracy:  0.5334\n",
      "Epoch  709 Batch size:  10000 Batch loss:  -1.2476125e+19 Training accuracy:  0.5334\n",
      "Epoch  710 Batch size:  10000 Batch loss:  -1.2567412e+19 Training accuracy:  0.5334\n",
      "Epoch  711 Batch size:  10000 Batch loss:  -1.2659107e+19 Training accuracy:  0.5334\n",
      "Epoch  712 Batch size:  10000 Batch loss:  -1.2751327e+19 Training accuracy:  0.5334\n",
      "Epoch  713 Batch size:  10000 Batch loss:  -1.2844145e+19 Training accuracy:  0.5334\n",
      "Epoch  714 Batch size:  10000 Batch loss:  -1.2937347e+19 Training accuracy:  0.5334\n",
      "Epoch  715 Batch size:  10000 Batch loss:  -1.3031174e+19 Training accuracy:  0.5334\n",
      "Epoch  716 Batch size:  10000 Batch loss:  -1.3125515e+19 Training accuracy:  0.5334\n",
      "Epoch  717 Batch size:  10000 Batch loss:  -1.3220345e+19 Training accuracy:  0.5334\n",
      "Epoch  718 Batch size:  10000 Batch loss:  -1.3315771e+19 Training accuracy:  0.5334\n",
      "Epoch  719 Batch size:  10000 Batch loss:  -1.3411654e+19 Training accuracy:  0.5334\n",
      "Epoch  720 Batch size:  10000 Batch loss:  -1.3508059e+19 Training accuracy:  0.5334\n",
      "Epoch  721 Batch size:  10000 Batch loss:  -1.3605086e+19 Training accuracy:  0.5334\n",
      "Epoch  722 Batch size:  10000 Batch loss:  -1.3702678e+19 Training accuracy:  0.5334\n",
      "Epoch  723 Batch size:  10000 Batch loss:  -1.3800734e+19 Training accuracy:  0.5334\n",
      "Epoch  724 Batch size:  10000 Batch loss:  -1.3899256e+19 Training accuracy:  0.5334\n",
      "Epoch  725 Batch size:  10000 Batch loss:  -1.3998345e+19 Training accuracy:  0.5334\n",
      "Epoch  726 Batch size:  10000 Batch loss:  -1.4098055e+19 Training accuracy:  0.5334\n",
      "Epoch  727 Batch size:  10000 Batch loss:  -1.4198338e+19 Training accuracy:  0.5334\n",
      "Epoch  728 Batch size:  10000 Batch loss:  -1.4299165e+19 Training accuracy:  0.5334\n",
      "Epoch  729 Batch size:  10000 Batch loss:  -1.4400548e+19 Training accuracy:  0.5334\n",
      "Epoch  730 Batch size:  10000 Batch loss:  -1.4502488e+19 Training accuracy:  0.5334\n",
      "Epoch  731 Batch size:  10000 Batch loss:  -1.4604971e+19 Training accuracy:  0.5334\n",
      "Epoch  732 Batch size:  10000 Batch loss:  -1.4708004e+19 Training accuracy:  0.5334\n",
      "Epoch  733 Batch size:  10000 Batch loss:  -1.4811585e+19 Training accuracy:  0.5334\n",
      "Epoch  734 Batch size:  10000 Batch loss:  -1.4915722e+19 Training accuracy:  0.5334\n",
      "Epoch  735 Batch size:  10000 Batch loss:  -1.5020411e+19 Training accuracy:  0.5334\n",
      "Epoch  736 Batch size:  10000 Batch loss:  -1.5125646e+19 Training accuracy:  0.5334\n",
      "Epoch  737 Batch size:  10000 Batch loss:  -1.5231428e+19 Training accuracy:  0.5334\n",
      "Epoch  738 Batch size:  10000 Batch loss:  -1.5337779e+19 Training accuracy:  0.5334\n",
      "Epoch  739 Batch size:  10000 Batch loss:  -1.5444733e+19 Training accuracy:  0.5334\n",
      "Epoch  740 Batch size:  10000 Batch loss:  -1.5552324e+19 Training accuracy:  0.5334\n",
      "Epoch  741 Batch size:  10000 Batch loss:  -1.5660565e+19 Training accuracy:  0.5334\n",
      "Epoch  742 Batch size:  10000 Batch loss:  -1.5769379e+19 Training accuracy:  0.5334\n",
      "Epoch  743 Batch size:  10000 Batch loss:  -1.5878651e+19 Training accuracy:  0.5334\n",
      "Epoch  744 Batch size:  10000 Batch loss:  -1.5988434e+19 Training accuracy:  0.5334\n",
      "Epoch  745 Batch size:  10000 Batch loss:  -1.6098896e+19 Training accuracy:  0.5334\n",
      "Epoch  746 Batch size:  10000 Batch loss:  -1.6210033e+19 Training accuracy:  0.5334\n",
      "Epoch  747 Batch size:  10000 Batch loss:  -1.6321737e+19 Training accuracy:  0.5334\n",
      "Epoch  748 Batch size:  10000 Batch loss:  -1.6433923e+19 Training accuracy:  0.5334\n",
      "Epoch  749 Batch size:  10000 Batch loss:  -1.6546728e+19 Training accuracy:  0.5334\n",
      "Epoch  750 Batch size:  10000 Batch loss:  -1.6660276e+19 Training accuracy:  0.5334\n",
      "Epoch  751 Batch size:  10000 Batch loss:  -1.6774273e+19 Training accuracy:  0.5334\n",
      "Epoch  752 Batch size:  10000 Batch loss:  -1.6888855e+19 Training accuracy:  0.5334\n",
      "Epoch  753 Batch size:  10000 Batch loss:  -1.7004244e+19 Training accuracy:  0.5334\n",
      "Epoch  754 Batch size:  10000 Batch loss:  -1.7119946e+19 Training accuracy:  0.5334\n",
      "Epoch  755 Batch size:  10000 Batch loss:  -1.7236548e+19 Training accuracy:  0.5334\n",
      "Epoch  756 Batch size:  10000 Batch loss:  -1.7353518e+19 Training accuracy:  0.5334\n",
      "Epoch  757 Batch size:  10000 Batch loss:  -1.7471295e+19 Training accuracy:  0.5334\n",
      "Epoch  758 Batch size:  10000 Batch loss:  -1.7589522e+19 Training accuracy:  0.5334\n",
      "Epoch  759 Batch size:  10000 Batch loss:  -1.7708525e+19 Training accuracy:  0.5334\n",
      "Epoch  760 Batch size:  10000 Batch loss:  -1.7827965e+19 Training accuracy:  0.5334\n",
      "Epoch  761 Batch size:  10000 Batch loss:  -1.7948246e+19 Training accuracy:  0.5334\n",
      "Epoch  762 Batch size:  10000 Batch loss:  -1.8068921e+19 Training accuracy:  0.5334\n",
      "Epoch  763 Batch size:  10000 Batch loss:  -1.819041e+19 Training accuracy:  0.5334\n",
      "Epoch  764 Batch size:  10000 Batch loss:  -1.8312465e+19 Training accuracy:  0.5334\n",
      "Epoch  765 Batch size:  10000 Batch loss:  -1.843506e+19 Training accuracy:  0.5334\n",
      "Epoch  766 Batch size:  10000 Batch loss:  -1.8558421e+19 Training accuracy:  0.5334\n",
      "Epoch  767 Batch size:  10000 Batch loss:  -1.8682394e+19 Training accuracy:  0.5334\n",
      "Epoch  768 Batch size:  10000 Batch loss:  -1.880694e+19 Training accuracy:  0.5334\n",
      "Epoch  769 Batch size:  10000 Batch loss:  -1.8932156e+19 Training accuracy:  0.5334\n",
      "Epoch  770 Batch size:  10000 Batch loss:  -1.905799e+19 Training accuracy:  0.5334\n",
      "Epoch  771 Batch size:  10000 Batch loss:  -1.9184492e+19 Training accuracy:  0.5334\n",
      "Epoch  772 Batch size:  10000 Batch loss:  -1.9311697e+19 Training accuracy:  0.5334\n",
      "Epoch  773 Batch size:  10000 Batch loss:  -1.9439563e+19 Training accuracy:  0.5334\n",
      "Epoch  774 Batch size:  10000 Batch loss:  -1.9568044e+19 Training accuracy:  0.5334\n",
      "Epoch  775 Batch size:  10000 Batch loss:  -1.9697124e+19 Training accuracy:  0.5334\n",
      "Epoch  776 Batch size:  10000 Batch loss:  -1.9826833e+19 Training accuracy:  0.5334\n",
      "Epoch  777 Batch size:  10000 Batch loss:  -1.995721e+19 Training accuracy:  0.5334\n",
      "Epoch  778 Batch size:  10000 Batch loss:  -2.0088262e+19 Training accuracy:  0.5334\n",
      "Epoch  779 Batch size:  10000 Batch loss:  -2.022e+19 Training accuracy:  0.5334\n",
      "Epoch  780 Batch size:  10000 Batch loss:  -2.035241e+19 Training accuracy:  0.5334\n",
      "Epoch  781 Batch size:  10000 Batch loss:  -2.0485505e+19 Training accuracy:  0.5334\n",
      "Epoch  782 Batch size:  10000 Batch loss:  -2.0619278e+19 Training accuracy:  0.5334\n",
      "Epoch  783 Batch size:  10000 Batch loss:  -2.0753722e+19 Training accuracy:  0.5334\n",
      "Epoch  784 Batch size:  10000 Batch loss:  -2.0888845e+19 Training accuracy:  0.5334\n",
      "Epoch  785 Batch size:  10000 Batch loss:  -2.1024637e+19 Training accuracy:  0.5334\n",
      "Epoch  786 Batch size:  10000 Batch loss:  -2.1161089e+19 Training accuracy:  0.5334\n",
      "Epoch  787 Batch size:  10000 Batch loss:  -2.1298196e+19 Training accuracy:  0.5334\n",
      "Epoch  788 Batch size:  10000 Batch loss:  -2.1435989e+19 Training accuracy:  0.5334\n",
      "Epoch  789 Batch size:  10000 Batch loss:  -2.1574494e+19 Training accuracy:  0.5334\n",
      "Epoch  790 Batch size:  10000 Batch loss:  -2.1713732e+19 Training accuracy:  0.5334\n",
      "Epoch  791 Batch size:  10000 Batch loss:  -2.185363e+19 Training accuracy:  0.5334\n",
      "Epoch  792 Batch size:  10000 Batch loss:  -2.199416e+19 Training accuracy:  0.5334\n",
      "Epoch  793 Batch size:  10000 Batch loss:  -2.2135419e+19 Training accuracy:  0.5334\n",
      "Epoch  794 Batch size:  10000 Batch loss:  -2.2277394e+19 Training accuracy:  0.5334\n",
      "Epoch  795 Batch size:  10000 Batch loss:  -2.242003e+19 Training accuracy:  0.5334\n",
      "Epoch  796 Batch size:  10000 Batch loss:  -2.2563448e+19 Training accuracy:  0.5334\n",
      "Epoch  797 Batch size:  10000 Batch loss:  -2.2707583e+19 Training accuracy:  0.5334\n",
      "Epoch  798 Batch size:  10000 Batch loss:  -2.2852327e+19 Training accuracy:  0.5334\n",
      "Epoch  799 Batch size:  10000 Batch loss:  -2.2997783e+19 Training accuracy:  0.5334\n",
      "Epoch  800 Batch size:  10000 Batch loss:  -2.314396e+19 Training accuracy:  0.5334\n",
      "Epoch  801 Batch size:  10000 Batch loss:  -2.329096e+19 Training accuracy:  0.5334\n",
      "Epoch  802 Batch size:  10000 Batch loss:  -2.3438663e+19 Training accuracy:  0.5334\n",
      "Epoch  803 Batch size:  10000 Batch loss:  -2.3586895e+19 Training accuracy:  0.5334\n",
      "Epoch  804 Batch size:  10000 Batch loss:  -2.3735956e+19 Training accuracy:  0.5334\n",
      "Epoch  805 Batch size:  10000 Batch loss:  -2.388589e+19 Training accuracy:  0.5334\n",
      "Epoch  806 Batch size:  10000 Batch loss:  -2.4036408e+19 Training accuracy:  0.5334\n",
      "Epoch  807 Batch size:  10000 Batch loss:  -2.4187543e+19 Training accuracy:  0.5334\n",
      "Epoch  808 Batch size:  10000 Batch loss:  -2.4339645e+19 Training accuracy:  0.5334\n",
      "Epoch  809 Batch size:  10000 Batch loss:  -2.4492429e+19 Training accuracy:  0.5334\n",
      "Epoch  810 Batch size:  10000 Batch loss:  -2.4645714e+19 Training accuracy:  0.5334\n",
      "Epoch  811 Batch size:  10000 Batch loss:  -2.4800087e+19 Training accuracy:  0.5334\n",
      "Epoch  812 Batch size:  10000 Batch loss:  -2.4954973e+19 Training accuracy:  0.5334\n",
      "Epoch  813 Batch size:  10000 Batch loss:  -2.5110594e+19 Training accuracy:  0.5334\n",
      "Epoch  814 Batch size:  10000 Batch loss:  -2.5267206e+19 Training accuracy:  0.5334\n",
      "Epoch  815 Batch size:  10000 Batch loss:  -2.5424153e+19 Training accuracy:  0.5334\n",
      "Epoch  816 Batch size:  10000 Batch loss:  -2.5582304e+19 Training accuracy:  0.5334\n",
      "Epoch  817 Batch size:  10000 Batch loss:  -2.5740845e+19 Training accuracy:  0.5334\n",
      "Epoch  818 Batch size:  10000 Batch loss:  -2.590037e+19 Training accuracy:  0.5334\n",
      "Epoch  819 Batch size:  10000 Batch loss:  -2.6060567e+19 Training accuracy:  0.5334\n",
      "Epoch  820 Batch size:  10000 Batch loss:  -2.6221485e+19 Training accuracy:  0.5334\n",
      "Epoch  821 Batch size:  10000 Batch loss:  -2.6383298e+19 Training accuracy:  0.5334\n",
      "Epoch  822 Batch size:  10000 Batch loss:  -2.6545685e+19 Training accuracy:  0.5334\n",
      "Epoch  823 Batch size:  10000 Batch loss:  -2.6709057e+19 Training accuracy:  0.5334\n",
      "Epoch  824 Batch size:  10000 Batch loss:  -2.6873e+19 Training accuracy:  0.5334\n",
      "Epoch  825 Batch size:  10000 Batch loss:  -2.7037864e+19 Training accuracy:  0.5334\n",
      "Epoch  826 Batch size:  10000 Batch loss:  -2.7203453e+19 Training accuracy:  0.5334\n",
      "Epoch  827 Batch size:  10000 Batch loss:  -2.7369743e+19 Training accuracy:  0.5334\n",
      "Epoch  828 Batch size:  10000 Batch loss:  -2.7537064e+19 Training accuracy:  0.5334\n",
      "Epoch  829 Batch size:  10000 Batch loss:  -2.7704735e+19 Training accuracy:  0.5334\n",
      "Epoch  830 Batch size:  10000 Batch loss:  -2.7873746e+19 Training accuracy:  0.5334\n",
      "Epoch  831 Batch size:  10000 Batch loss:  -2.8042974e+19 Training accuracy:  0.5334\n",
      "Epoch  832 Batch size:  10000 Batch loss:  -2.8213396e+19 Training accuracy:  0.5334\n",
      "Epoch  833 Batch size:  10000 Batch loss:  -2.8384557e+19 Training accuracy:  0.5334\n",
      "Epoch  834 Batch size:  10000 Batch loss:  -2.855622e+19 Training accuracy:  0.5334\n",
      "Epoch  835 Batch size:  10000 Batch loss:  -2.8729128e+19 Training accuracy:  0.5334\n",
      "Epoch  836 Batch size:  10000 Batch loss:  -2.8902535e+19 Training accuracy:  0.5334\n",
      "Epoch  837 Batch size:  10000 Batch loss:  -2.9076715e+19 Training accuracy:  0.5334\n",
      "Epoch  838 Batch size:  10000 Batch loss:  -2.925199e+19 Training accuracy:  0.5334\n",
      "Epoch  839 Batch size:  10000 Batch loss:  -2.942785e+19 Training accuracy:  0.5334\n",
      "Epoch  840 Batch size:  10000 Batch loss:  -2.960444e+19 Training accuracy:  0.5334\n",
      "Epoch  841 Batch size:  10000 Batch loss:  -2.9782082e+19 Training accuracy:  0.5334\n",
      "Epoch  842 Batch size:  10000 Batch loss:  -2.9960463e+19 Training accuracy:  0.5334\n",
      "Epoch  843 Batch size:  10000 Batch loss:  -3.0139527e+19 Training accuracy:  0.5334\n",
      "Epoch  844 Batch size:  10000 Batch loss:  -3.0319537e+19 Training accuracy:  0.5334\n",
      "Epoch  845 Batch size:  10000 Batch loss:  -3.050043e+19 Training accuracy:  0.5334\n",
      "Epoch  846 Batch size:  10000 Batch loss:  -3.0682008e+19 Training accuracy:  0.5334\n",
      "Epoch  847 Batch size:  10000 Batch loss:  -3.0864428e+19 Training accuracy:  0.5334\n",
      "Epoch  848 Batch size:  10000 Batch loss:  -3.1047708e+19 Training accuracy:  0.5334\n",
      "Epoch  849 Batch size:  10000 Batch loss:  -3.123196e+19 Training accuracy:  0.5334\n",
      "Epoch  850 Batch size:  10000 Batch loss:  -3.141702e+19 Training accuracy:  0.5334\n",
      "Epoch  851 Batch size:  10000 Batch loss:  -3.1602781e+19 Training accuracy:  0.5334\n",
      "Epoch  852 Batch size:  10000 Batch loss:  -3.178934e+19 Training accuracy:  0.5334\n",
      "Epoch  853 Batch size:  10000 Batch loss:  -3.1976754e+19 Training accuracy:  0.5334\n",
      "Epoch  854 Batch size:  10000 Batch loss:  -3.2165065e+19 Training accuracy:  0.5334\n",
      "Epoch  855 Batch size:  10000 Batch loss:  -3.2354282e+19 Training accuracy:  0.5334\n",
      "Epoch  856 Batch size:  10000 Batch loss:  -3.2544392e+19 Training accuracy:  0.5334\n",
      "Epoch  857 Batch size:  10000 Batch loss:  -3.2735346e+19 Training accuracy:  0.5334\n",
      "Epoch  858 Batch size:  10000 Batch loss:  -3.2927136e+19 Training accuracy:  0.5334\n",
      "Epoch  859 Batch size:  10000 Batch loss:  -3.3119764e+19 Training accuracy:  0.5334\n",
      "Epoch  860 Batch size:  10000 Batch loss:  -3.3313256e+19 Training accuracy:  0.5334\n",
      "Epoch  861 Batch size:  10000 Batch loss:  -3.35076e+19 Training accuracy:  0.5334\n",
      "Epoch  862 Batch size:  10000 Batch loss:  -3.3702813e+19 Training accuracy:  0.5334\n",
      "Epoch  863 Batch size:  10000 Batch loss:  -3.389889e+19 Training accuracy:  0.5334\n",
      "Epoch  864 Batch size:  10000 Batch loss:  -3.4095838e+19 Training accuracy:  0.5334\n",
      "Epoch  865 Batch size:  10000 Batch loss:  -3.4293647e+19 Training accuracy:  0.5334\n",
      "Epoch  866 Batch size:  10000 Batch loss:  -3.4492335e+19 Training accuracy:  0.5334\n",
      "Epoch  867 Batch size:  10000 Batch loss:  -3.4691905e+19 Training accuracy:  0.5334\n",
      "Epoch  868 Batch size:  10000 Batch loss:  -3.489241e+19 Training accuracy:  0.5334\n",
      "Epoch  869 Batch size:  10000 Batch loss:  -3.5093882e+19 Training accuracy:  0.5334\n",
      "Epoch  870 Batch size:  10000 Batch loss:  -3.529629e+19 Training accuracy:  0.5334\n",
      "Epoch  871 Batch size:  10000 Batch loss:  -3.5499578e+19 Training accuracy:  0.5334\n",
      "Epoch  872 Batch size:  10000 Batch loss:  -3.5703687e+19 Training accuracy:  0.5334\n",
      "Epoch  873 Batch size:  10000 Batch loss:  -3.590859e+19 Training accuracy:  0.5334\n",
      "Epoch  874 Batch size:  10000 Batch loss:  -3.6114302e+19 Training accuracy:  0.5334\n",
      "Epoch  875 Batch size:  10000 Batch loss:  -3.6320904e+19 Training accuracy:  0.5334\n",
      "Epoch  876 Batch size:  10000 Batch loss:  -3.652861e+19 Training accuracy:  0.5334\n",
      "Epoch  877 Batch size:  10000 Batch loss:  -3.6737404e+19 Training accuracy:  0.5334\n",
      "Epoch  878 Batch size:  10000 Batch loss:  -3.694698e+19 Training accuracy:  0.5334\n",
      "Epoch  879 Batch size:  10000 Batch loss:  -3.7157345e+19 Training accuracy:  0.5334\n",
      "Epoch  880 Batch size:  10000 Batch loss:  -3.7368367e+19 Training accuracy:  0.5334\n",
      "Epoch  881 Batch size:  10000 Batch loss:  -3.7580766e+19 Training accuracy:  0.5334\n",
      "Epoch  882 Batch size:  10000 Batch loss:  -3.7794094e+19 Training accuracy:  0.5334\n",
      "Epoch  883 Batch size:  10000 Batch loss:  -3.800818e+19 Training accuracy:  0.5334\n",
      "Epoch  884 Batch size:  10000 Batch loss:  -3.8222908e+19 Training accuracy:  0.5334\n",
      "Epoch  885 Batch size:  10000 Batch loss:  -3.843908e+19 Training accuracy:  0.5334\n",
      "Epoch  886 Batch size:  10000 Batch loss:  -3.8656115e+19 Training accuracy:  0.5334\n",
      "Epoch  887 Batch size:  10000 Batch loss:  -3.8873594e+19 Training accuracy:  0.5334\n",
      "Epoch  888 Batch size:  10000 Batch loss:  -3.909257e+19 Training accuracy:  0.5334\n",
      "Epoch  889 Batch size:  10000 Batch loss:  -3.9312326e+19 Training accuracy:  0.5334\n",
      "Epoch  890 Batch size:  10000 Batch loss:  -3.9532773e+19 Training accuracy:  0.5334\n",
      "Epoch  891 Batch size:  10000 Batch loss:  -3.975458e+19 Training accuracy:  0.5334\n",
      "Epoch  892 Batch size:  10000 Batch loss:  -3.9976963e+19 Training accuracy:  0.5334\n",
      "Epoch  893 Batch size:  10000 Batch loss:  -4.0200577e+19 Training accuracy:  0.5334\n",
      "Epoch  894 Batch size:  10000 Batch loss:  -4.0424988e+19 Training accuracy:  0.5334\n",
      "Epoch  895 Batch size:  10000 Batch loss:  -4.065039e+19 Training accuracy:  0.5334\n",
      "Epoch  896 Batch size:  10000 Batch loss:  -4.0876794e+19 Training accuracy:  0.5334\n",
      "Epoch  897 Batch size:  10000 Batch loss:  -4.1104086e+19 Training accuracy:  0.5334\n",
      "Epoch  898 Batch size:  10000 Batch loss:  -4.1332436e+19 Training accuracy:  0.5334\n",
      "Epoch  899 Batch size:  10000 Batch loss:  -4.1561676e+19 Training accuracy:  0.5334\n",
      "Epoch  900 Batch size:  10000 Batch loss:  -4.179196e+19 Training accuracy:  0.5334\n",
      "Epoch  901 Batch size:  10000 Batch loss:  -4.2023137e+19 Training accuracy:  0.5334\n",
      "Epoch  902 Batch size:  10000 Batch loss:  -4.2255424e+19 Training accuracy:  0.5334\n",
      "Epoch  903 Batch size:  10000 Batch loss:  -4.2488507e+19 Training accuracy:  0.5334\n",
      "Epoch  904 Batch size:  10000 Batch loss:  -4.272284e+19 Training accuracy:  0.5334\n",
      "Epoch  905 Batch size:  10000 Batch loss:  -4.295787e+19 Training accuracy:  0.5334\n",
      "Epoch  906 Batch size:  10000 Batch loss:  -4.319405e+19 Training accuracy:  0.5334\n",
      "Epoch  907 Batch size:  10000 Batch loss:  -4.343129e+19 Training accuracy:  0.5334\n",
      "Epoch  908 Batch size:  10000 Batch loss:  -4.366915e+19 Training accuracy:  0.5334\n",
      "Epoch  909 Batch size:  10000 Batch loss:  -4.390857e+19 Training accuracy:  0.5334\n",
      "Epoch  910 Batch size:  10000 Batch loss:  -4.4148673e+19 Training accuracy:  0.5334\n",
      "Epoch  911 Batch size:  10000 Batch loss:  -4.4389497e+19 Training accuracy:  0.5334\n",
      "Epoch  912 Batch size:  10000 Batch loss:  -4.463199e+19 Training accuracy:  0.5334\n",
      "Epoch  913 Batch size:  10000 Batch loss:  -4.4875177e+19 Training accuracy:  0.5334\n",
      "Epoch  914 Batch size:  10000 Batch loss:  -4.511898e+19 Training accuracy:  0.5334\n",
      "Epoch  915 Batch size:  10000 Batch loss:  -4.536428e+19 Training accuracy:  0.5334\n",
      "Epoch  916 Batch size:  10000 Batch loss:  -4.5610795e+19 Training accuracy:  0.5334\n",
      "Epoch  917 Batch size:  10000 Batch loss:  -4.5857872e+19 Training accuracy:  0.5334\n",
      "Epoch  918 Batch size:  10000 Batch loss:  -4.610579e+19 Training accuracy:  0.5334\n",
      "Epoch  919 Batch size:  10000 Batch loss:  -4.6355124e+19 Training accuracy:  0.5334\n",
      "Epoch  920 Batch size:  10000 Batch loss:  -4.6605716e+19 Training accuracy:  0.5334\n",
      "Epoch  921 Batch size:  10000 Batch loss:  -4.6857205e+19 Training accuracy:  0.5334\n",
      "Epoch  922 Batch size:  10000 Batch loss:  -4.7109403e+19 Training accuracy:  0.5334\n",
      "Epoch  923 Batch size:  10000 Batch loss:  -4.7362528e+19 Training accuracy:  0.5334\n",
      "Epoch  924 Batch size:  10000 Batch loss:  -4.761686e+19 Training accuracy:  0.5334\n",
      "Epoch  925 Batch size:  10000 Batch loss:  -4.787242e+19 Training accuracy:  0.5334\n",
      "Epoch  926 Batch size:  10000 Batch loss:  -4.8129094e+19 Training accuracy:  0.5334\n",
      "Epoch  927 Batch size:  10000 Batch loss:  -4.8386855e+19 Training accuracy:  0.5334\n",
      "Epoch  928 Batch size:  10000 Batch loss:  -4.8645605e+19 Training accuracy:  0.5334\n",
      "Epoch  929 Batch size:  10000 Batch loss:  -4.890536e+19 Training accuracy:  0.5334\n",
      "Epoch  930 Batch size:  10000 Batch loss:  -4.9166127e+19 Training accuracy:  0.5334\n",
      "Epoch  931 Batch size:  10000 Batch loss:  -4.942791e+19 Training accuracy:  0.5334\n",
      "Epoch  932 Batch size:  10000 Batch loss:  -4.969079e+19 Training accuracy:  0.5334\n",
      "Epoch  933 Batch size:  10000 Batch loss:  -4.995471e+19 Training accuracy:  0.5334\n",
      "Epoch  934 Batch size:  10000 Batch loss:  -5.0219727e+19 Training accuracy:  0.5334\n",
      "Epoch  935 Batch size:  10000 Batch loss:  -5.04858e+19 Training accuracy:  0.5334\n",
      "Epoch  936 Batch size:  10000 Batch loss:  -5.075297e+19 Training accuracy:  0.5334\n",
      "Epoch  937 Batch size:  10000 Batch loss:  -5.1021205e+19 Training accuracy:  0.5334\n",
      "Epoch  938 Batch size:  10000 Batch loss:  -5.1290454e+19 Training accuracy:  0.5334\n",
      "Epoch  939 Batch size:  10000 Batch loss:  -5.156084e+19 Training accuracy:  0.5334\n",
      "Epoch  940 Batch size:  10000 Batch loss:  -5.183219e+19 Training accuracy:  0.5334\n",
      "Epoch  941 Batch size:  10000 Batch loss:  -5.2104796e+19 Training accuracy:  0.5334\n",
      "Epoch  942 Batch size:  10000 Batch loss:  -5.237834e+19 Training accuracy:  0.5334\n",
      "Epoch  943 Batch size:  10000 Batch loss:  -5.2653197e+19 Training accuracy:  0.5334\n",
      "Epoch  944 Batch size:  10000 Batch loss:  -5.2929113e+19 Training accuracy:  0.5334\n",
      "Epoch  945 Batch size:  10000 Batch loss:  -5.320621e+19 Training accuracy:  0.5334\n",
      "Epoch  946 Batch size:  10000 Batch loss:  -5.3484406e+19 Training accuracy:  0.5334\n",
      "Epoch  947 Batch size:  10000 Batch loss:  -5.3763405e+19 Training accuracy:  0.5334\n",
      "Epoch  948 Batch size:  10000 Batch loss:  -5.4043714e+19 Training accuracy:  0.5334\n",
      "Epoch  949 Batch size:  10000 Batch loss:  -5.4325106e+19 Training accuracy:  0.5334\n",
      "Epoch  950 Batch size:  10000 Batch loss:  -5.460777e+19 Training accuracy:  0.5334\n",
      "Epoch  951 Batch size:  10000 Batch loss:  -5.489151e+19 Training accuracy:  0.5334\n",
      "Epoch  952 Batch size:  10000 Batch loss:  -5.5176027e+19 Training accuracy:  0.5334\n",
      "Epoch  953 Batch size:  10000 Batch loss:  -5.5462023e+19 Training accuracy:  0.5334\n",
      "Epoch  954 Batch size:  10000 Batch loss:  -5.5749303e+19 Training accuracy:  0.5334\n",
      "Epoch  955 Batch size:  10000 Batch loss:  -5.6037195e+19 Training accuracy:  0.5334\n",
      "Epoch  956 Batch size:  10000 Batch loss:  -5.6326543e+19 Training accuracy:  0.5334\n",
      "Epoch  957 Batch size:  10000 Batch loss:  -5.661725e+19 Training accuracy:  0.5334\n",
      "Epoch  958 Batch size:  10000 Batch loss:  -5.690857e+19 Training accuracy:  0.5334\n",
      "Epoch  959 Batch size:  10000 Batch loss:  -5.7201327e+19 Training accuracy:  0.5334\n",
      "Epoch  960 Batch size:  10000 Batch loss:  -5.7495416e+19 Training accuracy:  0.5334\n",
      "Epoch  961 Batch size:  10000 Batch loss:  -5.7790327e+19 Training accuracy:  0.5334\n",
      "Epoch  962 Batch size:  10000 Batch loss:  -5.808641e+19 Training accuracy:  0.5334\n",
      "Epoch  963 Batch size:  10000 Batch loss:  -5.8383843e+19 Training accuracy:  0.5334\n",
      "Epoch  964 Batch size:  10000 Batch loss:  -5.868253e+19 Training accuracy:  0.5334\n",
      "Epoch  965 Batch size:  10000 Batch loss:  -5.898202e+19 Training accuracy:  0.5334\n",
      "Epoch  966 Batch size:  10000 Batch loss:  -5.9282747e+19 Training accuracy:  0.5334\n",
      "Epoch  967 Batch size:  10000 Batch loss:  -5.9584875e+19 Training accuracy:  0.5334\n",
      "Epoch  968 Batch size:  10000 Batch loss:  -5.9888235e+19 Training accuracy:  0.5334\n",
      "Epoch  969 Batch size:  10000 Batch loss:  -6.0192474e+19 Training accuracy:  0.5334\n",
      "Epoch  970 Batch size:  10000 Batch loss:  -6.049778e+19 Training accuracy:  0.5334\n",
      "Epoch  971 Batch size:  10000 Batch loss:  -6.0804625e+19 Training accuracy:  0.5334\n",
      "Epoch  972 Batch size:  10000 Batch loss:  -6.1112664e+19 Training accuracy:  0.5334\n",
      "Epoch  973 Batch size:  10000 Batch loss:  -6.1421776e+19 Training accuracy:  0.5334\n",
      "Epoch  974 Batch size:  10000 Batch loss:  -6.1732e+19 Training accuracy:  0.5334\n",
      "Epoch  975 Batch size:  10000 Batch loss:  -6.2043233e+19 Training accuracy:  0.5334\n",
      "Epoch  976 Batch size:  10000 Batch loss:  -6.235599e+19 Training accuracy:  0.5334\n",
      "Epoch  977 Batch size:  10000 Batch loss:  -6.2670003e+19 Training accuracy:  0.5334\n",
      "Epoch  978 Batch size:  10000 Batch loss:  -6.298518e+19 Training accuracy:  0.5334\n",
      "Epoch  979 Batch size:  10000 Batch loss:  -6.3301343e+19 Training accuracy:  0.5334\n",
      "Epoch  980 Batch size:  10000 Batch loss:  -6.3618816e+19 Training accuracy:  0.5334\n",
      "Epoch  981 Batch size:  10000 Batch loss:  -6.393746e+19 Training accuracy:  0.5334\n",
      "Epoch  982 Batch size:  10000 Batch loss:  -6.425727e+19 Training accuracy:  0.5334\n",
      "Epoch  983 Batch size:  10000 Batch loss:  -6.4578456e+19 Training accuracy:  0.5334\n",
      "Epoch  984 Batch size:  10000 Batch loss:  -6.4900956e+19 Training accuracy:  0.5334\n",
      "Epoch  985 Batch size:  10000 Batch loss:  -6.5224683e+19 Training accuracy:  0.5334\n",
      "Epoch  986 Batch size:  10000 Batch loss:  -6.554949e+19 Training accuracy:  0.5334\n",
      "Epoch  987 Batch size:  10000 Batch loss:  -6.5875414e+19 Training accuracy:  0.5334\n",
      "Epoch  988 Batch size:  10000 Batch loss:  -6.6202554e+19 Training accuracy:  0.5334\n",
      "Epoch  989 Batch size:  10000 Batch loss:  -6.6531013e+19 Training accuracy:  0.5334\n",
      "Epoch  990 Batch size:  10000 Batch loss:  -6.686077e+19 Training accuracy:  0.5334\n",
      "Epoch  991 Batch size:  10000 Batch loss:  -6.719179e+19 Training accuracy:  0.5334\n",
      "Epoch  992 Batch size:  10000 Batch loss:  -6.7524017e+19 Training accuracy:  0.5334\n",
      "Epoch  993 Batch size:  10000 Batch loss:  -6.785753e+19 Training accuracy:  0.5334\n",
      "Epoch  994 Batch size:  10000 Batch loss:  -6.819228e+19 Training accuracy:  0.5334\n",
      "Epoch  995 Batch size:  10000 Batch loss:  -6.8528285e+19 Training accuracy:  0.5334\n",
      "Epoch  996 Batch size:  10000 Batch loss:  -6.886555e+19 Training accuracy:  0.5334\n",
      "Epoch  997 Batch size:  10000 Batch loss:  -6.9204075e+19 Training accuracy:  0.5334\n",
      "Epoch  998 Batch size:  10000 Batch loss:  -6.9543877e+19 Training accuracy:  0.5334\n",
      "Epoch  999 Batch size:  10000 Batch loss:  -6.988493e+19 Training accuracy:  0.5334\n",
      "Finish!\n",
      "Store model weights and biases\n",
      "drive/My Drive/Coding/GCN/data/weights.pkl\n",
      "drive/My Drive/Coding/GCN/data/biases.pkl\n",
      "Testing accuracy:  0.4875 Batch size:  10000\n"
     ]
    }
   ],
   "source": [
    "# Start trainings session\n",
    "init  = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for e in range(1000):\n",
    "        \n",
    "        batch_x = f # one hot for each node (word + docs) in the graph\n",
    "        batch_y = np.eye(n_classes)[y_train]\n",
    "        \n",
    "        sess.run(optimizer, feed_dict = {X: batch_x,\n",
    "                                         y: batch_y,\n",
    "                                         keep_prob: dropout,\n",
    "                                         idx_selected: idx_train})\n",
    "        \n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict = {X: batch_x,\n",
    "                                                            y: batch_y,\n",
    "                                                            keep_prob: 1.,\n",
    "                                                            idx_selected: idx_train})\n",
    "        \n",
    "        print(\"Epoch \", e, \"Batch size: \", batch_y.shape[0] ,\"Batch loss: \", loss, \"Training accuracy: \", acc)\n",
    "    \n",
    "    # save_path = saver.save(sess, \"drive/My Drive/Coding/GCN/data/model.ckpt\")\n",
    "\n",
    "    print(\"Finish!\")\n",
    "\n",
    "    print(\"Store model weights and biases\")\n",
    "    weights_dict = {}\n",
    "    for key, values in weights.items():\n",
    "        weights_dict[key] = sess.run(values)\n",
    "    save_as_pickle(\"weights.pkl\", weights_dict)\n",
    "\n",
    "    biases_dict = {}\n",
    "    for key, values in biases.items():\n",
    "        biases_dict[key] = sess.run(values)\n",
    "    save_as_pickle(\"biases.pkl\", biases_dict)\n",
    "\n",
    "    # Calculate acc for test docs\n",
    "    batch_x = f\n",
    "    batch_y = np.eye(n_classes)[y_test]\n",
    "    test_acc = sess.run(accuracy, feed_dict = {X: batch_x,\n",
    "                                               y: batch_y,\n",
    "                                               keep_prob: 1.,\n",
    "                                               idx_selected: idx_test})\n",
    "    print(\"Testing accuracy: \", test_acc, \"Batch size: \", batch_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iUtpYnvEQIK5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PRUJo4QiQINn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wjwlywrpQIQq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FN3wHodAQITk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zF0bkDKUQIWp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "authorship_tag": "ABX9TyOxt3RaQdVSb8hJLY1pbheq",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "gcn_imdb.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
